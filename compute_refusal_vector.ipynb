{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.notebook import tqdm\n",
    "from livelossplot import PlotLosses \n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "# import os\n",
    "\n",
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "# from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Samuel's directory instead\n",
    "model_path: str = \"/home/samuel/research/llmattacks/llm-attacks/DIR/llama-2/llama/Llama-2-7b-chat-hf\"\n",
    "device: str = \"cuda:0\"\n",
    "num_steps: int = 200\n",
    "num_tokens: int = 300 \n",
    "step_size: float = 0.1\n",
    "\n",
    "seed: int = 42\n",
    "# load_dataset = True\n",
    "# verbose = True\n",
    "# early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA 2 Chat model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Padding settings for LLaMA 2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Helper functions\n",
    "def get_harmful_instructions():\n",
    "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "    response = requests.get(url)\n",
    "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "    instructions = dataset['goal'].tolist()\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def get_harmless_instructions():\n",
    "    hf_path = 'tatsu-lab/alpaca'\n",
    "    dataset = load_dataset(hf_path)\n",
    "    instructions = []\n",
    "    for i in range(len(dataset['train'])):\n",
    "        if dataset['train'][i]['input'].strip() == '':\n",
    "            instructions.append(dataset['train'][i]['instruction'])\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Harmful instructions:\", len(harmful_inst_train))\n",
    "for i in range(5,10):\n",
    "    print(f\"\\t{repr(harmful_inst_train[i])}\")\n",
    "print(\"Harmless instructions:\", len(harmful_inst_train))\n",
    "for i in range(5,10):\n",
    "    print(f\"\\t{repr(harmless_inst_train[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_CHAT_TEMPLATE = \"\"\"<s>[INST] {instruction} [/INST]\"\"\"\n",
    "\n",
    "def tokenize_instructions_llama_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: list\n",
    ") -> torch.Tensor:\n",
    "    prompts = [LLAMA_CHAT_TEMPLATE.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(tokenize_instructions_llama_chat, tokenizer=tokenizer)\n",
    "\n",
    "def generate_with_model(\n",
    "    model,\n",
    "    toks,\n",
    "    max_tokens_generated: int = 64\n",
    "):\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "    for i in range(max_tokens_generated):\n",
    "        outputs = model(input_ids=all_toks[:, :toks.shape[1] + i])\n",
    "        logits = outputs.logits\n",
    "        next_tokens = logits[:, -1, :].argmax(dim=-1)\n",
    "        all_toks[:, toks.shape[1] + i] = next_tokens\n",
    "    return tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def get_generations(\n",
    "    model,\n",
    "    instructions: list,\n",
    "    tokenize_instructions_fn: callable,\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 4\n",
    ") -> list:\n",
    "    generations = []\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "        generation = generate_with_model(model, toks, max_tokens_generated=max_tokens_generated)\n",
    "        generations.extend(generation)\n",
    "    return generations\n",
    "\n",
    "\n",
    "# def get_generations(\n",
    "#     model: HookedTransformer,\n",
    "#     instructions: List[str],\n",
    "#     tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "#     fwd_hooks = [],\n",
    "#     max_tokens_generated: int = 64,\n",
    "#     batch_size: int = 4,\n",
    "# ) -> List[str]:\n",
    "\n",
    "#     generations = []\n",
    "\n",
    "#     for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "#         toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "#         generation = _generate_with_hooks(\n",
    "#             model,\n",
    "#             toks,\n",
    "#             max_tokens_generated=max_tokens_generated,\n",
    "#             fwd_hooks=fwd_hooks,\n",
    "#         )\n",
    "#         generations.extend(generation)\n",
    "\n",
    "#     return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "harmful_inst_train, _ = get_harmful_instructions()\n",
    "harmless_inst_train, _ = get_harmless_instructions()\n",
    "\n",
    "N_INST_TRAIN = 32\n",
    "\n",
    "# Tokenize instructions\n",
    "harmful_toks = tokenize_instructions_fn(instructions=harmful_inst_train[:N_INST_TRAIN])\n",
    "harmless_toks = tokenize_instructions_fn(instructions=harmless_inst_train[:N_INST_TRAIN])\n",
    "\n",
    "# Compute activations and refusal direction\n",
    "with torch.no_grad():\n",
    "    harmful_outputs = model(input_ids=harmful_toks, output_hidden_states=True)\n",
    "    harmless_outputs = model(input_ids=harmless_toks, output_hidden_states=True)\n",
    "\n",
    "    layer = 14  # Replace this with the layer to inspect\n",
    "    pos = -1    # Position to use (e.g., last token)\n",
    "\n",
    "    harmful_mean_act = harmful_outputs.hidden_states[layer][:, pos, :].mean(dim=0)\n",
    "    harmless_mean_act = harmless_outputs.hidden_states[layer][:, pos, :].mean(dim=0)\n",
    "\n",
    "refusal_dir = harmful_mean_act - harmless_mean_act\n",
    "refusal_dir = refusal_dir / refusal_dir.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "del harmful_outputs, harmless_outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, Int\n",
    "def direction_ablation_hook(\n",
    "    activation: Float[Tensor, \"... d_act\"],\n",
    "    hook: HookPoint,\n",
    "    direction: Float[Tensor, \"d_act\"]\n",
    "):\n",
    "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation - proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INST_TEST = 32\n",
    "intervention_dir = refusal_dir\n",
    "# intervention_layers = list(range(model.cfg.n_layers)) # all layers\n",
    "# Determine the number of layers in the LlamaForCausalLM model\n",
    "intervention_layers = list(range(model.config.num_hidden_layers))  # Replace `cfg` with `config`\n",
    "\n",
    "hook_fn = functools.partial(direction_ablation_hook,direction=intervention_dir)\n",
    "fwd_hooks = [(utils.get_act_name(act_name, l), hook_fn) for l in intervention_layers for act_name in ['resid_pre', 'resid_mid', 'resid_post']]\n",
    "\n",
    "intervention_generations = get_generations(model, harmful_inst_test[:N_INST_TEST], tokenize_instructions_fn) #, fwd_hooks=fwd_hooks)\n",
    "baseline_generations = get_generations(model, harmful_inst_test[:N_INST_TEST], tokenize_instructions_fn) #, fwd_hooks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function for ablating direction\n",
    "def direction_ablation_hook(\n",
    "    activation: Float[torch.Tensor, \"... d_act\"],\n",
    "    hook,\n",
    "    direction: Float[torch.Tensor, \"d_act\"]\n",
    "):\n",
    "    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "    return activation - proj\n",
    "\n",
    "# Number of instructions to test\n",
    "N_INST_TEST = 32\n",
    "intervention_dir = refusal_dir\n",
    "\n",
    "# Determine the number of layers in the LlamaForCausalLM model\n",
    "intervention_layers = list(range(model.config.num_hidden_layers))  # Replace `cfg` with `config`\n",
    "\n",
    "# Define the hook function\n",
    "hook_fn = functools.partial(direction_ablation_hook, direction=intervention_dir)\n",
    "\n",
    "# Set forward hooks for relevant layers and activation points\n",
    "fwd_hooks = [\n",
    "    (f\"transformer.h.{l}.input_layernorm\", hook_fn) for l in intervention_layers\n",
    "] + [\n",
    "    (f\"transformer.h.{l}.post_attention_layernorm\", hook_fn) for l in intervention_layers\n",
    "]\n",
    "\n",
    "# Generate outputs with interventions\n",
    "intervention_generations = get_generations(\n",
    "    model,\n",
    "    harmful_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Generate baseline outputs without interventions\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    harmful_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
    "    print(f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(f\"INTERVENTION COMPLETION:\")\n",
    "    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    # print(Fore.RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Qwen 8B Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import functools\n",
    "# import einops\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# import io\n",
    "# import textwrap\n",
    "# import gc\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "# from torch import Tensor\n",
    "# from typing import List, Callable\n",
    "# from transformer_lens import HookedTransformer, utils\n",
    "# from transformer_lens.hook_points import HookPoint\n",
    "# from transformers import AutoTokenizer\n",
    "# from jaxtyping import Float, Int\n",
    "# # from colorama import Fore\n",
    "\n",
    "\n",
    "# MODEL_PATH = 'Qwen/Qwen-1_8B-chat'\n",
    "# DEVICE = 'cuda'\n",
    "\n",
    "# model = HookedTransformer.from_pretrained_no_processing(\n",
    "#     MODEL_PATH,\n",
    "#     device=DEVICE,\n",
    "#     dtype=torch.float16,\n",
    "#     default_padding_side='left',\n",
    "#     fp16=True\n",
    "# )\n",
    "\n",
    "# model.tokenizer.padding_side = 'left'\n",
    "# model.tokenizer.pad_token = '<|extra_0|>'\n",
    "\n",
    "\n",
    "# def get_harmful_instructions():\n",
    "#     url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "#     instructions = dataset['goal'].tolist()\n",
    "\n",
    "#     train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "#     return train, test\n",
    "\n",
    "# def get_harmless_instructions():\n",
    "#     hf_path = 'tatsu-lab/alpaca'\n",
    "#     dataset = load_dataset(hf_path)\n",
    "\n",
    "#     # filter for instructions that do not have inputs\n",
    "#     instructions = []\n",
    "#     for i in range(len(dataset['train'])):\n",
    "#         if dataset['train'][i]['input'].strip() == '':\n",
    "#             instructions.append(dataset['train'][i]['instruction'])\n",
    "\n",
    "#     train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "# harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "# harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "\n",
    "\n",
    "# QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user\n",
    "# {instruction}<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "\n",
    "# def tokenize_instructions_qwen_chat(\n",
    "#     tokenizer: AutoTokenizer,\n",
    "#     instructions: List[str]\n",
    "# ) -> Int[Tensor, 'batch_size seq_len']:\n",
    "#     prompts = [QWEN_CHAT_TEMPLATE.format(instruction=instruction) for instruction in instructions]\n",
    "#     return tokenizer(prompts, padding=True,truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# tokenize_instructions_fn = functools.partial(tokenize_instructions_qwen_chat, tokenizer=model.tokenizer)\n",
    "\n",
    "\n",
    "# def _generate_with_hooks(\n",
    "#     model: HookedTransformer,\n",
    "#     toks: Int[Tensor, 'batch_size seq_len'],\n",
    "#     max_tokens_generated: int = 64,\n",
    "#     fwd_hooks = [],\n",
    "# ) -> List[str]:\n",
    "\n",
    "#     all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
    "#     all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "#     for i in range(max_tokens_generated):\n",
    "#         with model.hooks(fwd_hooks=fwd_hooks):\n",
    "#             logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "#             next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
    "#             all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "#     return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# def get_generations(\n",
    "#     model: HookedTransformer,\n",
    "#     instructions: List[str],\n",
    "#     tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "#     fwd_hooks = [],\n",
    "#     max_tokens_generated: int = 64,\n",
    "#     batch_size: int = 4,\n",
    "# ) -> List[str]:\n",
    "\n",
    "#     generations = []\n",
    "\n",
    "#     for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "#         toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "#         generation = _generate_with_hooks(\n",
    "#             model,\n",
    "#             toks,\n",
    "#             max_tokens_generated=max_tokens_generated,\n",
    "#             fwd_hooks=fwd_hooks,\n",
    "#         )\n",
    "#         generations.extend(generation)\n",
    "\n",
    "#     return generations\n",
    "\n",
    "# N_INST_TRAIN = 32\n",
    "\n",
    "# # tokenize instructions\n",
    "# harmful_toks = tokenize_instructions_fn(instructions=harmful_inst_train[:N_INST_TRAIN])\n",
    "# harmless_toks = tokenize_instructions_fn(instructions=harmless_inst_train[:N_INST_TRAIN])\n",
    "\n",
    "# # run model on harmful and harmless instructions, caching intermediate activations\n",
    "# harmful_logits, harmful_cache = model.run_with_cache(harmful_toks, names_filter=lambda hook_name: 'resid' in hook_name)\n",
    "# harmless_logits, harmless_cache = model.run_with_cache(harmless_toks, names_filter=lambda hook_name: 'resid' in hook_name)\n",
    "\n",
    "# # compute difference of means between harmful and harmless activations at an intermediate layer\n",
    "\n",
    "# pos = -1\n",
    "# layer = 14\n",
    "\n",
    "# harmful_mean_act = harmful_cache['resid_pre', layer][:, pos, :].mean(dim=0)\n",
    "# harmless_mean_act = harmless_cache['resid_pre', layer][:, pos, :].mean(dim=0)\n",
    "\n",
    "# refusal_dir = harmful_mean_act - harmless_mean_act\n",
    "# refusal_dir = refusal_dir / refusal_dir.norm()\n",
    "\n",
    "# # clean up memory\n",
    "# del harmful_cache, harmless_cache, harmful_logits, harmless_logits\n",
    "# gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "# def direction_ablation_hook(\n",
    "#     activation: Float[Tensor, \"... d_act\"],\n",
    "#     hook: HookPoint,\n",
    "#     direction: Float[Tensor, \"d_act\"]\n",
    "# ):\n",
    "#     proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction\n",
    "#     return activation - proj\n",
    "\n",
    "# N_INST_TEST = 32\n",
    "# intervention_dir = refusal_dir\n",
    "# intervention_layers = list(range(model.cfg.n_layers)) # all layers\n",
    "\n",
    "# hook_fn = functools.partial(direction_ablation_hook,direction=intervention_dir)\n",
    "# fwd_hooks = [(utils.get_act_name(act_name, l), hook_fn) for l in intervention_layers for act_name in ['resid_pre', 'resid_mid', 'resid_post']]\n",
    "\n",
    "# intervention_generations = get_generations(model, harmful_inst_test[:N_INST_TEST], tokenize_instructions_fn, fwd_hooks=fwd_hooks)\n",
    "# baseline_generations = get_generations(model, harmful_inst_test[:N_INST_TEST], tokenize_instructions_fn, fwd_hooks=[])\n",
    "\n",
    "# for i in range(N_INST_TEST):\n",
    "#     print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
    "#     print(f\"BASELINE COMPLETION:\")\n",
    "#     print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "#     print(f\"INTERVENTION COMPLETION:\")\n",
    "#     print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "#     # print(Fore.RESET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-embedding-attack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
