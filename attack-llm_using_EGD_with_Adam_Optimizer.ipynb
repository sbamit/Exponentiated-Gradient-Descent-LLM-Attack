{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS and GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.notebook import tqdm\n",
    "from livelossplot import PlotLosses \n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "\n",
    "# use Samuel's directory instead\n",
    "model_path: str = \"/home/samuel/research/llmattacks/llm-attacks/DIR/llama-2/llama/Llama-2-7b-chat-hf\"\n",
    "device: str = \"cuda:0\"\n",
    "num_steps: int = 200\n",
    "num_tokens: int = 300 \n",
    "step_size: float = 0.1\n",
    "\n",
    "seed: int = 42\n",
    "load_csv_data = True\n",
    "# verbose = True\n",
    "# early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path, tokenizer_path=None, device=\"cuda:0\", **kwargs):\n",
    "    # from llm-attacks\n",
    "    model = (\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, trust_remote_code=True, **kwargs\n",
    "        ).to(device).eval()\n",
    "    )\n",
    "\n",
    "    tokenizer_path = model_path if tokenizer_path is None else tokenizer_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path, trust_remote_code=True, use_fast=False\n",
    "    )\n",
    "\n",
    "    if \"llama-2\" in tokenizer_path:\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_embedding_matrix(model):\n",
    "    # from llm-attacks\n",
    "    if isinstance(model, LlamaForCausalLM):\n",
    "        return model.model.embed_tokens.weight\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        model_path, low_cpu_mem_usage=True, use_cache=False, device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(input_string):\n",
    "    return torch.tensor(tokenizer(input_string)[\"input_ids\"], device=device)\n",
    "\n",
    "\n",
    "def create_one_hot_and_embeddings(tokens, embed_weights):\n",
    "    one_hot = torch.zeros(\n",
    "        tokens.shape[0], embed_weights.shape[0], device=device, dtype=embed_weights.dtype\n",
    "    )\n",
    "    one_hot.scatter_(\n",
    "        1,\n",
    "        tokens.unsqueeze(1),\n",
    "        torch.ones(one_hot.shape[0], 1, device=device, dtype=embed_weights.dtype),\n",
    "    )\n",
    "    embeddings = (one_hot @ embed_weights).unsqueeze(0).data\n",
    "    return one_hot, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonascii_toks(tokenizer):\n",
    "    def is_ascii(s):\n",
    "        return s.isascii() and s.isprintable()\n",
    "    non_ascii_toks = []\n",
    "    for i in range(3, tokenizer.vocab_size):\n",
    "        if not is_ascii(tokenizer.decode([i])):\n",
    "            non_ascii_toks.append(i)\n",
    "    \n",
    "    if tokenizer.bos_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.bos_token_id)\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.eos_token_id)\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.pad_token_id)\n",
    "    if tokenizer.unk_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.unk_token_id)\n",
    "    \n",
    "    return torch.tensor(non_ascii_toks).to(device)\n",
    "\n",
    "non_ascii_toks = get_nonascii_toks(tokenizer).tolist()\n",
    "# Assuming device is already defined\n",
    "non_ascii_toks_tensor = torch.tensor(non_ascii_toks).to(device=device)\n",
    "\n",
    "\n",
    "def get_masked_one_hot_adv(one_hot_adv):\n",
    "    # Mask out all the non-ascii tokens and then return\n",
    "    # Step 1: Create a tensor with all the non_ascii tokens\n",
    "    top_token_ids_tensor_2d = non_ascii_toks_tensor.unsqueeze(0).repeat(20, 1)\n",
    "    \n",
    "    # Step 2: Create a mask with the same shape as one_hot_adv, initialized to zero\n",
    "    mask = torch.ones_like(one_hot_adv, dtype=torch.float16)\n",
    "\n",
    "    # Step 3: Use token_ids_init to set the corresponding indices in the mask to 1\n",
    "    # We use gather and scatter operations for this\n",
    "    mask.scatter_(1, top_token_ids_tensor_2d, 0.0)\n",
    "\n",
    "    # Step 4: Apply the mask to one_hot_adv\n",
    "    masked_one_hot_adv = one_hot_adv * mask\n",
    "    \n",
    "    return masked_one_hot_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, targets):\n",
    "    full_embeddings = torch.hstack([embeddings_user, embeddings_adv, embeddings_target])\n",
    "    logits = model(inputs_embeds=full_embeddings).logits\n",
    "    loss_slice_start = len(embeddings_user[0]) + len(embeddings_adv[0])\n",
    "    loss = nn.CrossEntropyLoss()(logits[0, loss_slice_start - 1 : -1, :], targets)\n",
    "    return loss, logits[:, loss_slice_start-1:, :]\n",
    "    # return loss, logits.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGDwithAdamOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-4, grad_clip_val=1.0):\n",
    "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps=eps, grad_clip_val=grad_clip_val)\n",
    "        super(EGDwithAdamOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "        # Initialize state variables for each parameter group\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                self.state[param] = {\n",
    "                    'm': torch.zeros_like(param),  # First moment vector (momentum)\n",
    "                    'v': torch.zeros_like(param),  # Second moment vector (variance)\n",
    "                    't': 0                         # Time step\n",
    "                }\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            eps = group['eps']\n",
    "            # grad_clip_val = group['grad_clip_val']\n",
    "\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad\n",
    "                state = self.state[param]\n",
    "\n",
    "                # Retrieve state variables\n",
    "                m = state['m']\n",
    "                v = state['v']\n",
    "                t = state['t']\n",
    "\n",
    "                # Increment time step\n",
    "                t += 1\n",
    "                state['t'] = t\n",
    "\n",
    "                # # Clip gradients to avoid exploding updates; Redundant step\n",
    "                # grad = torch.clamp(grad, min=-grad_clip_val, max=grad_clip_val)\n",
    "\n",
    "                # Update biased first moment estimate (m) and second moment estimate (v)     \n",
    "                m = beta1 * m + (1 - beta1) * grad\n",
    "                v = beta2 * v + (1 - beta2) * torch.pow(grad, 2) \n",
    "                # v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "                state['m'] = m\n",
    "                state['v'] = v\n",
    "\n",
    "                # Bias correction\n",
    "                m_hat = m / (1 - math.pow(beta1, t)) # m_hat = m / (1 - beta1 ** t)\n",
    "                v_hat = v / (1 - math.pow(beta2, t)) # v_hat = v / (1 - beta2 ** t)\n",
    "\n",
    "                # Adam-like modified gradient\n",
    "                modified_grad = m_hat / (torch.sqrt(v_hat) + eps)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Exponentiated Gradient Descent update with Adam-like modifications\n",
    "                    param.mul_(torch.exp(-lr * modified_grad))\n",
    "\n",
    "                    # Clamp the parameter values to avoid extreme values\n",
    "                    param.clamp_(min=1e-12, max=1e12)\n",
    "\n",
    "                    # Normalize each row to sum up to 1, considering possible tensor shapes\n",
    "                    if param.dim() > 1:\n",
    "                        row_sums = param.sum(dim=1, keepdim=True) + 1e-10  # Add epsilon to avoid division by zero\n",
    "                        param.div_(row_sums)\n",
    "                    else:\n",
    "                        # Handle 1D tensors or other cases if applicable\n",
    "                        param.div_(param.sum() + 1e-10)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN ATTACK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "dataset_name = 'MaliciousInstruct'\n",
    "dataset_path: str = './data/'+dataset_name+'/harmful_behaviors.csv'\n",
    "if load_csv_data:\n",
    "    reader = csv.reader(open(dataset_path, 'r'))\n",
    "    next(reader)\n",
    "else:\n",
    "    print(\"Error!\")\n",
    "harmful_behaviors = list(reader)[0:50]\n",
    "# We can hard-code this safe_target string; probably\n",
    "# safe_target: str = \"I cannot fulfill your request. I'm just an AI.\"\n",
    "print(len(harmful_behaviors))\n",
    "for each_behavior in harmful_behaviors:\n",
    "    print(each_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the one_hot encodings using Gradient Descent.\n",
    "from math import *\n",
    "import json \n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fetch the Model's weights\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "# reader = [[user_prompt, target]]\n",
    "optimization_results = []\n",
    "# Set your regularization parameter\n",
    "initial_coefficient = 1e-5\n",
    "final_coefficient = 1e-3\n",
    "\n",
    "\n",
    "# Create the following directories if they do NOT exist\n",
    "directories = [\"./CSV_Files\", f'./CSV_Files/{dataset_name}', \"./JSON_Files\", f'./JSON_Files/{dataset_name}']\n",
    "for directory in directories:\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "    \n",
    "# JSON file to dump the model generated outputs \n",
    "json_file_path = \"./JSON_Files/\"+dataset_name+\"/output_EGD_with_Adam_Optimizer(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).json\"\n",
    "\n",
    "for row in tqdm(harmful_behaviors, desc=\"Optimizing prompts\"):\n",
    "    iteration_result = {}\n",
    "    user_prompt, target = row\n",
    "    print(user_prompt)\n",
    "    # target = unsafe_target\n",
    "    user_prompt_tokens = get_tokens(user_prompt)    \n",
    "    target_tokens = get_tokens(target)[1:]\n",
    "    plotlosses = PlotLosses()\n",
    "    plotlosses = PlotLosses(groups={'loss': ['continuous_loss', 'discrete_loss']})    \n",
    "    target_tokens = get_tokens(target)[1:]\n",
    "\n",
    "    # Initialize one_hot encodings and embedding vectors for the user prompts and targets \n",
    "    one_hot_inputs, embeddings_user = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights)\n",
    "    one_hot_target, embeddings_target = create_one_hot_and_embeddings(target_tokens, embed_weights)\n",
    "    best_disc_loss = np.inf\n",
    "    best_loss_at_epoch = 0\n",
    "    # # Initialize the adversarial one_hot encodings using ONLY ASCII tokens\n",
    "    # one_hot_adv = get_only_ASCII_one_hot_adv()\n",
    "    # # Do NOT Initialize the adversarial one_hot encodings using all '!' tokens\n",
    "    # one_hot_adv = get_all_exclamations_one_hot_adv(embed_weights=embed_weights)\n",
    "    # You can also initialize one_hot_adv Randomly.\n",
    "    one_hot_adv = F.softmax(torch.rand(20, 32000, dtype=torch.float16).to(device=device), dim=1)\n",
    "    one_hot_adv.requires_grad_() \n",
    "    # Initialize the optimizer\n",
    "    optimizer = EGDwithAdamOptimizer([one_hot_adv], lr=step_size)\n",
    "    # Initialize the learning_rate scheduler\n",
    "    scheduler_cycle = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=50)\n",
    "    # Specify the filename\n",
    "    last_three_words = lambda text: '_'.join(text.split()[-3:])\n",
    "    first_three_words = lambda text: '_'.join(text.split()[0:3])\n",
    "    csv_filename = './CSV_Files/'+dataset_name+'/output_EGD_with_Adam_Optimizer('+first_three_words(user_prompt)+\"..\"+last_three_words(user_prompt)+')('+str(num_steps)+'_steps).csv'        \n",
    "    # Generate column names\n",
    "    column_names = ['epoch', 'cycle', 'learning_rate', 'entropy_term', \n",
    "                    'kl_divergence_term', \n",
    "                    'continuous_loss','discrete_loss'] \n",
    "    # Adding 'max_1' to 'max_20' column names using a loop\n",
    "    for i in range(1, 21):\n",
    "        column_names.append(f'max_{i}')\n",
    "    for i in range(1, 21):\n",
    "        column_names.append(f'token_id_{i}')\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    # PHASE_2: bool=False\n",
    "    BREAK_IT: bool=False\n",
    "\n",
    "    for epoch_no in tqdm(range(num_steps)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings_adv = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "        cross_entropy_loss, _ = calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, one_hot_target)\n",
    "        continuous_loss = cross_entropy_loss.detach().cpu().item()\n",
    "        # Break the loop if loss becomes \"NaN\"\n",
    "        if math.isnan(continuous_loss):\n",
    "            print(\"The value is NaN.\")\n",
    "            BREAK_IT=True # Go to the Next Behavior\n",
    "\n",
    "        # # Discretization part\n",
    "        masked_one_hot_adv = get_masked_one_hot_adv(one_hot_adv)\n",
    "        # Use this masked_one_hot_adv ONLY for discretization\n",
    "        max_values = torch.max(masked_one_hot_adv, dim=1)\n",
    "        adv_token_ids = max_values.indices\n",
    "        # adv_token_ids = top_k_sampling(one_hot_adv, k=10).squeeze(dim=1)\n",
    "        # tau = initial_tau * (final_tau / initial_tau) ** (epoch_no / (num_steps - 1))\n",
    "        # one_hot_discrete = F.gumbel_softmax(one_hot_adv, tau=tau, hard=True)\n",
    "        # adv_token_ids = one_hot_discrete.argmax(dim=1)\n",
    "        one_hot_discrete = torch.zeros(\n",
    "            adv_token_ids.shape[0], embed_weights.shape[0], device=device, dtype=embed_weights.dtype\n",
    "        )\n",
    "        one_hot_discrete.scatter_(\n",
    "            1,\n",
    "            adv_token_ids.unsqueeze(1),\n",
    "            torch.ones(masked_one_hot_adv.shape[0], 1, device=device, dtype=embed_weights.dtype),\n",
    "        )\n",
    "        # Use one_hot_discrete to print Tokens\n",
    "        # What other techniques Can we use here to discretize the one_hot encodings?\n",
    "        # Use discrete tokens to calculate loss\n",
    "        embeddings_adv_discrete = (one_hot_discrete @ embed_weights).unsqueeze(0)\n",
    "        disc_loss, _ = calc_loss(model, embeddings_user, embeddings_adv_discrete, embeddings_target, one_hot_target)\n",
    "        # If loss improves, save it as x_best\n",
    "        discrete_loss =  disc_loss.detach().cpu().item()\n",
    "        # cur_loss_list.append(cur_loss)\n",
    "        if discrete_loss < best_disc_loss:\n",
    "            # print(f\"########## {discrete_loss} #########\")\n",
    "            best_disc_loss = discrete_loss\n",
    "            best_loss_at_epoch = epoch_no\n",
    "            effective_adv_embeddings = embeddings_adv_discrete\n",
    "            effective_adv_one_hot = one_hot_discrete\n",
    "        else :\n",
    "            pass\n",
    "\n",
    "        ################# Usual entropy #################\n",
    "        # # Calculate H(X) = âˆ’ âˆ‘_{i=1}^{L} âˆ‘_{j=1}^{|T|} X_{ij} (log X_{ij} âˆ’1)\n",
    "        # Adding a small epsilon to avoid log(0) which is undefined\n",
    "        eps = 1e-12\n",
    "        one_hot_adv_float32 = one_hot_adv.to(dtype=torch.float32)\n",
    "        log_one_hot = torch.log(one_hot_adv_float32 + eps)\n",
    "        # Compute the modified entropy\n",
    "        entropy_per_row = -one_hot_adv_float32 * (log_one_hot - 1)\n",
    "        entropy_term = entropy_per_row.sum()\n",
    "        # Calculate the annealed coefficient, ðœ–\n",
    "        # Use Exponential Scheduling\n",
    "        reg_coefficient = initial_coefficient * (final_coefficient / initial_coefficient) ** (epoch_no / (num_steps - 1))   \n",
    "        entropy_term *= reg_coefficient \n",
    "        # Regularized loss: F(X) - epsilon * H(X)\n",
    "        regularized_loss = cross_entropy_loss - entropy_term\n",
    "        # Take the Negative log of the highest probability in each row\n",
    "        kl_divergence_term = -torch.log(torch.max(one_hot_adv, dim=1).values + eps).sum()\n",
    "        kl_divergence_term *=reg_coefficient\n",
    "        regularized_loss +=kl_divergence_term\n",
    "\n",
    "        # Backpropagate the regularized loss\n",
    "        regularized_loss.backward()\n",
    "\n",
    "        # Get the scheduler's state to get learning_rate\n",
    "        scheduler_state = scheduler.state_dict()\n",
    "\n",
    "        # Clip the gradients(before update)\n",
    "        torch.nn.utils.clip_grad_norm_([one_hot_adv], max_norm=1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # update scheduler\n",
    "        scheduler.step(continuous_loss)\n",
    "        # # Reset the gradients; redundant after calling optimizer.zero_grad()\n",
    "        # model.zero_grad()\n",
    "        # one_hot_adv.grad.zero_()\n",
    "        \n",
    "\n",
    "        # Dump Ouput values to a CSV file\n",
    "        # Convert max_values to a NumPy array\n",
    "        max_values_array = max_values.values.detach().cpu().numpy()\n",
    "        token_ids_array = adv_token_ids.detach().cpu().numpy()\n",
    "        # Get the scheduler's state and learning_rate\n",
    "        # Access the current learning rate directly from the optimizer's parameter group\n",
    "        scheduler_lr = optimizer.param_groups[0]['lr']  # Get the learning rate of the first parameter group\n",
    "        # scheduler_lr = scheduler_state['_last_lr'][0]\n",
    "        # Create the Initial array       \n",
    "        prepend_array = np.array([epoch_no, scheduler_cycle, scheduler_lr,\n",
    "                                  entropy_term.detach().cpu().item(), \n",
    "                                  kl_divergence_term.detach().cpu().item(),\n",
    "                                  continuous_loss, discrete_loss]) \n",
    "        \n",
    "        # Concatenate the arrays\n",
    "        row = np.concatenate((prepend_array, max_values_array, token_ids_array))\n",
    "        new_row = pd.Series(row, index=df.columns)\n",
    "        df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "        # # Save log data to CSV file periodically\n",
    "        # if epoch_no % 10 == 0:\n",
    "        #     df.to_csv(csv_filename, index=False)\n",
    "        # Something went wrong; so break it and go to the next behavior.\n",
    "        if BREAK_IT==True:\n",
    "            break\n",
    "    # End of optimizations        \n",
    "    # Write to CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    # Token and Output Generation Step\n",
    "    inputs_token_ids = one_hot_inputs.argmax(dim=1)\n",
    "    adv_token_ids = effective_adv_one_hot.argmax(dim=1)\n",
    "    adv_suffix_list = str(adv_token_ids.cpu().numpy().tolist())\n",
    "    adv_suffix_string = tokenizer.decode(adv_token_ids.cpu().numpy())\n",
    "    final_string_ids = torch.hstack([inputs_token_ids, adv_token_ids])\n",
    "    outputs = []\n",
    "    generation_loop = range(1)\n",
    "    import warnings\n",
    "    # Suppress specific warnings\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*`do_sample` is set to `False`. However, `temperature` is set to.*\")\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*`do_sample` is set to `False`. However, `top_p` is set to.*\")\n",
    "    \n",
    "    ######## Discrete tokens for Output Generation ########\n",
    "    for loop in tqdm(generation_loop, desc=\"Generating outputs\", leave=False):\n",
    "        generated_output = model.generate(final_string_ids.unsqueeze(0), \n",
    "                                          max_length=num_tokens, \n",
    "                                          pad_token_id=tokenizer.pad_token_id,\n",
    "                                          do_sample=False)\n",
    "        generated_output_string = tokenizer.decode(generated_output[0][:].cpu().numpy(), skip_special_tokens=True).strip()\n",
    "        outputs.append(generated_output_string)\n",
    "    \n",
    "    iteration_result = {\n",
    "        \"harmful-behaviour\": user_prompt,\n",
    "        \"target\": target,\n",
    "        \"suffix_token_ids\": adv_suffix_list,\n",
    "        \"suffix_string\": adv_suffix_string,\n",
    "        \"epoch_no\": epoch_no,\n",
    "        \"continuous_loss\": continuous_loss,\n",
    "        \"discrete_loss\": discrete_loss,\n",
    "        \"best_disc_loss\":best_disc_loss,\n",
    "        \"epch_at_best_disc_loss\": best_loss_at_epoch,\n",
    "        \"outputs\": outputs\n",
    "    }\n",
    "    optimization_results.append(iteration_result)\n",
    "\n",
    "    # Update the JSON File\n",
    "    with open(json_file_path, \"w\") as f:\n",
    "        json.dump(\n",
    "            optimization_results,\n",
    "            f,\n",
    "            indent=4,  # Keep structure readable\n",
    "            ensure_ascii=False  # Do this to Handle non-ASCII chars properly; If there is any\n",
    "        )\n",
    "\n",
    "    ##########################################################################################\n",
    "    ## Create and Update a JSONL file (required for running the parse_results.ipynb script) ##\n",
    "    output_file = \"./JSON_Files/\"+dataset_name+\"/EGD_with_Adam_Llama2(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).jsonl\"\n",
    "    from behavior import Behavior\n",
    "    behavior = Behavior(user_prompt, adv_suffix_string, generated_output_string, \"\", \"\")\n",
    "    \n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(json.dumps(behavior.to_dict()) + '\\n')\n",
    "    f.close()  \n",
    "    ## Create and Update a JSONL file (required for running the parse_results.ipynb script) ##\n",
    "    ########################################################################################## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Run this Block\n",
    "# def get_nonascii_toks(tokenizer):\n",
    "#     def is_ascii(s):\n",
    "#         return s.isascii() and s.isprintable()\n",
    "#     non_ascii_toks = []\n",
    "#     for i in range(3, tokenizer.vocab_size):\n",
    "#         if not is_ascii(tokenizer.decode([i])):\n",
    "#             non_ascii_toks.append(i)\n",
    "    \n",
    "#     if tokenizer.bos_token_id is not None:\n",
    "#         non_ascii_toks.append(tokenizer.bos_token_id)\n",
    "#     if tokenizer.eos_token_id is not None:\n",
    "#         non_ascii_toks.append(tokenizer.eos_token_id)\n",
    "#     if tokenizer.pad_token_id is not None:\n",
    "#         non_ascii_toks.append(tokenizer.pad_token_id)\n",
    "#     if tokenizer.unk_token_id is not None:\n",
    "#         non_ascii_toks.append(tokenizer.unk_token_id)\n",
    "    \n",
    "#     return torch.tensor(non_ascii_toks).to(device)\n",
    "\n",
    "# # # Test method\n",
    "# # non_ascii_toks = get_nonascii_toks(tokenizer)\n",
    "# # print(non_ascii_toks.tolist())\n",
    "\n",
    "\n",
    "# def get_only_ASCII_one_hot_adv():\n",
    "\n",
    "#     non_ascii_toks = get_nonascii_toks(tokenizer).tolist()\n",
    "#     no_of_ascii_toks = tokenizer.vocab_size - len(non_ascii_toks)\n",
    "#     # print(len(non_ascii_toks))\n",
    "#     # print(tokenizer.vocab_size)\n",
    "#     # print(no_of_ascii_toks)\n",
    "#     # for tok_id in non_ascii_toks:\n",
    "#     #     print(tokenizer.decode(tok_id), end=' ')\n",
    "#     # Assuming device is already defined\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     # Step 1: Create the 20x32000 tensor randomly initialized and Initialize weights at the center of the simplex\n",
    "#     one_hot_adv = torch.rand(20, 32000, dtype=torch.float16).to(device=device)\n",
    "#     # dims = one_hot_adv.size(-1)\n",
    "#     one_hot_adv.data.fill_(1 / no_of_ascii_toks)\n",
    "#     # Step 2: Assuming token_ids_init is a tensor of shape (20, k) where k is the number of indices per row\n",
    "#     # top_token_ids_tensor = select_topk_adv_token_ids(input_files, input_directory)\n",
    "#     # print('size:', top_token_ids_tensor.size())\n",
    "#     # print('token_ids:', top_token_ids_tensor)\n",
    "#     # Repeat the tensor 20 times along a new dimension and then reshape\n",
    "#     non_ascii_toks_tensor = torch.tensor(non_ascii_toks).to(device=device)\n",
    "#     top_token_ids_tensor_2d = non_ascii_toks_tensor.unsqueeze(0).repeat(20, 1)\n",
    "    \n",
    "#     # Step 3: Create a mask with the same shape as one_hot_adv, initialized to zero\n",
    "#     mask = torch.ones_like(one_hot_adv, dtype=torch.float16)\n",
    "\n",
    "#     # Step 4: Use token_ids_init to set the corresponding indices in the mask to 1\n",
    "#     # We use gather and scatter operations for this\n",
    "#     mask.scatter_(1, top_token_ids_tensor_2d, 0.0)\n",
    "\n",
    "#     # Step 5: Apply the mask to one_hot_adv\n",
    "#     one_hot_adv = one_hot_adv * mask\n",
    "    \n",
    "#     return one_hot_adv\n",
    "\n",
    "\n",
    "# def get_all_exclamations_one_hot_adv(embed_weights):\n",
    "#     adv_string_init: str = \"! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\"\n",
    "#     adv_string_tokens = get_tokens(adv_string_init)[1:]\n",
    "#     one_hot_adv, _ = create_one_hot_and_embeddings(adv_string_tokens, embed_weights)\n",
    "#     return one_hot_adv\n",
    "\n",
    "# # # Example Usage\n",
    "# # one_hot_adv = select_only_ASCII_token_ids()\n",
    "# # print(one_hot_adv)\n",
    "\n",
    "# # sum = torch.sum(one_hot_adv, dim=1)\n",
    "# # max = torch.max(one_hot_adv, dim=1)\n",
    "# # min = torch.min(one_hot_adv, dim=1)\n",
    "# # print(sum,',', max, ',', min)\n",
    "\n",
    "# def count_common_words(str1, str2):\n",
    "#     # Split the strings into words\n",
    "#     words1 = set(str1.split())\n",
    "#     words2 = set(str2.split())\n",
    "\n",
    "#     # Find the intersection of the two sets to get the common words\n",
    "#     common_words = words1.intersection(words2)\n",
    "\n",
    "#     # Return the number of common words\n",
    "#     return len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Update the JSON File\n",
    "    # with open(json_file_path, \"w\") as f:\n",
    "    #     json.dump(optimization_results, f, indent=4)\n",
    "\n",
    "    ######## Continuous tokens for Output Generation ########    \n",
    "    # embeddings_adv_projected = project_to_nearest_embeddings(embeddings_adv.squeeze(0), embed_weights)\n",
    "    # Project the relaxed adversarial embeddings to their nearest neighbors in the original embedding space using Euclidean distance\n",
    "    # embeddings_adv_projected, min_distances = project_to_nearest_embeddings_euclidean(embeddings_adv.squeeze(0), embed_weights)\n",
    "\n",
    "    # Use the projected embeddings for generation\n",
    "    # Make sure to unsqueeze the dimension to match the expected input shape for model.generate\n",
    "    # final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv_projected.unsqueeze(0)])\n",
    "\n",
    "    # Convert min_distances tensor to a list of numerical values and print\n",
    "    # min_distances_list = min_distances.cpu().tolist()\n",
    "    # print(\"Minimum Euclidean distances for each embedding:\", min_distances_list)\n",
    "    # Calculate and print the mean of all the Euclidean distances\n",
    "    # mean_distance = sum(min_distances_list) / len(min_distances_list)\n",
    "    # print(\"Mean of Euclidean distances:\", mean_distance)\n",
    "    \n",
    "    # # Use the projected embeddings for generation\n",
    "    # # Make sure to unsqueeze the dimension to match the expected input shape for model.generate\n",
    "    # final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv_projected.unsqueeze(0)])\n",
    "    # final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# class ExpGDOptimizer(torch.optim.Optimizer):\n",
    "#     def __init__(self, params, lr=0.01):\n",
    "#         defaults = dict(lr=lr)\n",
    "#         super(ExpGDOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "#     def step(self, closure=None):\n",
    "#         loss = None\n",
    "#         if closure is not None:\n",
    "#             loss = closure()\n",
    "\n",
    "#         for group in self.param_groups:\n",
    "#             for param in group['params']:\n",
    "#                 if param.grad is None:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Get the gradient and learning rate\n",
    "#                 grad = param.grad\n",
    "#                 lr = group['lr']\n",
    "                \n",
    "#                 # Exponentiated Gradient Descent update\n",
    "#                 # with torch.no_grad():\n",
    "#                 #     # one_hot_adv.data *= torch.exp(-eta * grads)\n",
    "#                 #     one_hot_adv.data *= torch.exp(-step_size * grads)\n",
    "#                 #     one_hot_adv.data /= one_hot_adv.data.sum(dim=1, keepdim=True)\n",
    "#                 # Apply the EGD update\n",
    "#                 with torch.no_grad():\n",
    "#                     param.mul_(torch.exp(-lr * grad))  # Exponentiated Gradient update\n",
    "#                     # Normalize each row to sum up to 1\n",
    "#                     row_sums = param.sum(dim=1, keepdim=True) + 1e-10  # Add epsilon to avoid division by zero\n",
    "#                     param.div_(row_sums)\n",
    "#         return loss\n",
    "\n",
    "# # # Example usage with a neural network\n",
    "# # model = torch.nn.Linear(10, 1)  # A simple linear model\n",
    "# # optimizer = EGDOptimizer(model.parameters(), lr=0.01)\n",
    "\n",
    "# # # Training loop\n",
    "# # for data, target in dataloader:\n",
    "# #     optimizer.zero_grad()\n",
    "# #     output = model(data)\n",
    "# #     loss = loss_fn(output, target)\n",
    "# #     loss.backward()\n",
    "# #     optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExpGDOptimizer(torch.optim.Optimizer):\n",
    "#     def __init__(self, params, lr=0.01):\n",
    "#         defaults = dict(lr=lr)\n",
    "#         super(ExpGDOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "#     def step(self, closure=None):\n",
    "#         loss = None\n",
    "#         if closure is not None:\n",
    "#             loss = closure()\n",
    "\n",
    "#         for group in self.param_groups:\n",
    "#             for param in group['params']:\n",
    "#                 if param.grad is None:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Get the gradient and learning rate\n",
    "#                 grad = param.grad\n",
    "#                 lr = group['lr']\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "#                     # Clip gradients to avoid exploding updates\n",
    "#                     grad = torch.clamp(grad, min=-10, max=10)\n",
    "#                     # Exponentiated Gradient Descent update\n",
    "#                     param.mul_(torch.exp(-lr * grad))\n",
    "#                     # Clamp the parameter values to avoid extreme values\n",
    "#                     param.clamp_(min=1e-6, max=1e6)\n",
    "#                     # Normalize each row to sum up to 1\n",
    "#                     row_sums = param.sum(dim=1, keepdim=True) + 1e-10  # Add epsilon to avoid division by zero\n",
    "#                     param.div_(row_sums)\n",
    "\n",
    "#         return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File paths to the .pt files\n",
    "\n",
    "# for row in harmful_behaviors:\n",
    "#     iteration_result = {}\n",
    "#     user_prompt, _ = row\n",
    "#     print(f'\\n\\n{user_prompt}')\n",
    "#     print('________________________________________________________________________________')\n",
    "#     tensor_file_unsafe = './TORCH_Files/EGD_w_Entropy_Unsafe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(unsafe_target)+'.pt'\n",
    "#     tensor_file_safe = './TORCH_Files/EGD_w_Entropy_Safe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(safe_target)+'.pt'\n",
    "#     tensor_filenames = [tensor_file_unsafe, tensor_file_safe]\n",
    "#     loaded_tensors = []\n",
    "#     skip: bool=False\n",
    "#     for filename in tensor_filenames:\n",
    "#         if os.path.exists(filename):\n",
    "#             loaded_tensors.append(torch.load(filename))\n",
    "#         else:\n",
    "#             print(f\"File {filename} does not exist. Skipping...\")\n",
    "#             skip=True\n",
    "#     if skip==True: # GO to the NExt Iteration\n",
    "#         continue\n",
    "#     # Load the tensors from the .pt files\n",
    "#     # loaded_tensors = [torch.load(filename) for filename in tensor_filenames]\n",
    "\n",
    "#     # Now, loaded_tensors is a list containing the loaded tensors\n",
    "#     tensor1 = loaded_tensors[0]\n",
    "#     tensor2 = loaded_tensors[1]\n",
    "\n",
    "#     # Move tensors to CPU and convert to float32 if necessary\n",
    "#     tensor1_cpu = tensor1.cpu().float()\n",
    "#     tensor2_cpu = tensor2.cpu().float()\n",
    "\n",
    "#     # Compute cosine similarity\n",
    "#     cosine_similarity = F.cosine_similarity(tensor1_cpu, tensor2_cpu, dim=1)\n",
    "\n",
    "#     # Compute L2 distance using torch.cdist\n",
    "#     l2_distance = torch.cdist(tensor1_cpu.unsqueeze(0), tensor2_cpu.unsqueeze(0), p=2).squeeze(0)\n",
    "\n",
    "#     # Print results\n",
    "#     print(\"Cosine Similarity:\")\n",
    "#     print(cosine_similarity)\n",
    "#     print(\"\\nL2-norm Distance size\")\n",
    "#     print(l2_distance.shape)\n",
    "\n",
    "#     # If you only need the distance between corresponding rows\n",
    "#     diagonal_l2_distance = l2_distance.diag()\n",
    "#     print(\"L2-norm Distance between corresponding rows:\")\n",
    "#     print(diagonal_l2_distance)\n",
    "\n",
    "#     # Compute L1 distance\n",
    "#     # Compute the L1-norm distance\n",
    "#     l1_distance = torch.cdist(tensor1_cpu.unsqueeze(0), tensor2_cpu.unsqueeze(0), p=1).squeeze(0)\n",
    "#     # torch.abs(tensor1 - tensor2).sum()\n",
    "#     print(\"\\nL1-norm distance size:\\n\", l1_distance.shape)\n",
    "#     print(\"L1-norm Distance between corresponding rows:\")\n",
    "#     print(l1_distance.diag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the tensor as a CSV file\n",
    "# def save_tensor_to_csv(tensor, filename):\n",
    "#     # Convert the tensor to a NumPy array\n",
    "#     numpy_array = tensor.cpu().numpy()\n",
    "#     # Convert the NumPy array to a DataFrame\n",
    "#     df = pd.DataFrame(numpy_array)\n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# csv_file_unsafe = './outputs/Unsafe_1_hot_adv_'+first_three_words(unsafe_target)+'.csv'\n",
    "# csv_file_safe = './outputs/Safe_1_hot_adv_'+first_three_words(safe_target)+'.csv'\n",
    "# save_tensor_to_csv(tensor1_cpu, csv_file_unsafe)\n",
    "# save_tensor_to_csv(tensor2_cpu, csv_file_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute cosine_similarity and l2-distance\n",
    "\n",
    "# # Move tensors to CPU if necessary\n",
    "# tensor1_cpu = one_hot_adv_list[0].cpu()\n",
    "# tensor2_cpu = one_hot_adv_list[1].cpu()\n",
    "\n",
    "# # Compute cosine similarity\n",
    "# cosine_similarity = F.cosine_similarity(tensor1_cpu, tensor2_cpu, dim=1)\n",
    "\n",
    "# # Compute L2 distance using torch.cdist\n",
    "# # Move tensors to CPU and convert to float32 if necessary\n",
    "# tensor1_cpu = tensor1_cpu.float()\n",
    "# tensor2_cpu = tensor2_cpu.float()\n",
    "# l2_distance = torch.cdist(tensor1_cpu, tensor2_cpu, p=2)\n",
    "# diagonal_l2_distance = l2_distance.diag()\n",
    "# # Print results\n",
    "# print(\"Cosine Similarity:\")\n",
    "# print(cosine_similarity)\n",
    "# print(\"\\nL2-norm Distance:\")\n",
    "# print(diagonal_l2_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After it's done, compute the similarity and distance between the one_hot_adv_Safe and one_hot_adv_Unsafe\n",
    "# print(len(one_hot_adv_list))\n",
    "# # Use the Last embeddings_adv\n",
    "# for one_hot_adv in one_hot_adv_list:\n",
    "#     embeddings_adv = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "#     final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv])\n",
    "#     outputs = []\n",
    "#     generation_loop = range(10)\n",
    "#     for i in tqdm(generation_loop, desc=\"Generating outputs\", leave=False):\n",
    "#         generated_output =  model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens).squeeze()\n",
    "#         generated_output_string = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "#         print(f\"#{i}\\n\",generated_output_string)\n",
    "#         outputs.append(generated_output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-embedding-attack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
