{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS and GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.notebook import tqdm\n",
    "from livelossplot import PlotLosses \n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "\n",
    "# use Samuel's directory instead\n",
    "model_path: str = \"/home/samuel/research/llmattacks/llm-attacks/DIR/llama-2/llama/Llama-2-7b-chat-hf\"\n",
    "device: str = \"cuda:0\"\n",
    "num_steps: int = 2000\n",
    "num_tokens: int = 100 # Make this smaller; such as 100\n",
    "step_size: float = 0.1 # BAL even using 0.1 does NOT solve the gradient explosion problem\n",
    "\n",
    "seed: int = 42\n",
    "load_dataset = True\n",
    "# verbose = True\n",
    "# early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path, tokenizer_path=None, device=\"cuda:0\", **kwargs):\n",
    "    # from llm-attacks\n",
    "    model = (\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, trust_remote_code=True, **kwargs\n",
    "        ).to(device).eval()\n",
    "    )\n",
    "\n",
    "    tokenizer_path = model_path if tokenizer_path is None else tokenizer_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path, trust_remote_code=True, use_fast=False\n",
    "    )\n",
    "\n",
    "    if \"llama-2\" in tokenizer_path:\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_embedding_matrix(model):\n",
    "    # from llm-attacks\n",
    "    if isinstance(model, LlamaForCausalLM):\n",
    "        return model.model.embed_tokens.weight\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        model_path, low_cpu_mem_usage=True, use_cache=False, device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(input_string):\n",
    "    return torch.tensor(tokenizer(input_string)[\"input_ids\"], device=device)\n",
    "\n",
    "\n",
    "def create_one_hot_and_embeddings(tokens, embed_weights):\n",
    "    one_hot = torch.zeros(\n",
    "        tokens.shape[0], embed_weights.shape[0], device=device, dtype=embed_weights.dtype\n",
    "    )\n",
    "    one_hot.scatter_(\n",
    "        1,\n",
    "        tokens.unsqueeze(1),\n",
    "        torch.ones(one_hot.shape[0], 1, device=device, dtype=embed_weights.dtype),\n",
    "    )\n",
    "    embeddings = (one_hot @ embed_weights).unsqueeze(0).data\n",
    "    return one_hot, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, targets):\n",
    "    full_embeddings = torch.hstack([embeddings_user, embeddings_adv, embeddings_target])\n",
    "    logits = model(inputs_embeds=full_embeddings).logits\n",
    "    loss_slice_start = len(embeddings_user[0]) + len(embeddings_adv[0])\n",
    "    loss = nn.CrossEntropyLoss()(logits[0, loss_slice_start - 1 : -1, :], targets)\n",
    "    return loss, logits[:, loss_slice_start-1:, :]\n",
    "    # return loss, logits.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonascii_toks(tokenizer):\n",
    "    def is_ascii(s):\n",
    "        return s.isascii() and s.isprintable()\n",
    "    non_ascii_toks = []\n",
    "    for i in range(3, tokenizer.vocab_size):\n",
    "        if not is_ascii(tokenizer.decode([i])):\n",
    "            non_ascii_toks.append(i)\n",
    "    \n",
    "    if tokenizer.bos_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.bos_token_id)\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.eos_token_id)\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.pad_token_id)\n",
    "    if tokenizer.unk_token_id is not None:\n",
    "        non_ascii_toks.append(tokenizer.unk_token_id)\n",
    "    \n",
    "    return torch.tensor(non_ascii_toks).to(device)\n",
    "\n",
    "# # Test method\n",
    "# non_ascii_toks = get_nonascii_toks(tokenizer)\n",
    "# print(non_ascii_toks.tolist())\n",
    "\n",
    "\n",
    "def get_only_ASCII_one_hot_adv():\n",
    "\n",
    "    non_ascii_toks = get_nonascii_toks(tokenizer).tolist()\n",
    "    no_of_ascii_toks = tokenizer.vocab_size - len(non_ascii_toks)\n",
    "    # print(len(non_ascii_toks))\n",
    "    # print(tokenizer.vocab_size)\n",
    "    # print(no_of_ascii_toks)\n",
    "    # for tok_id in non_ascii_toks:\n",
    "    #     print(tokenizer.decode(tok_id), end=' ')\n",
    "    # Assuming device is already defined\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Step 1: Create the 20x32000 tensor randomly initialized and Initialize weights at the center of the simplex\n",
    "    one_hot_adv = torch.rand(20, 32000, dtype=torch.float16).to(device=device)\n",
    "    # dims = one_hot_adv.size(-1)\n",
    "    one_hot_adv.data.fill_(1 / no_of_ascii_toks)\n",
    "    # Step 2: Assuming token_ids_init is a tensor of shape (20, k) where k is the number of indices per row\n",
    "    # top_token_ids_tensor = select_topk_adv_token_ids(input_files, input_directory)\n",
    "    # print('size:', top_token_ids_tensor.size())\n",
    "    # print('token_ids:', top_token_ids_tensor)\n",
    "    # Repeat the tensor 20 times along a new dimension and then reshape\n",
    "    non_ascii_toks_tensor = torch.tensor(non_ascii_toks).to(device=device)\n",
    "    top_token_ids_tensor_2d = non_ascii_toks_tensor.unsqueeze(0).repeat(20, 1)\n",
    "    \n",
    "    # Step 3: Create a mask with the same shape as one_hot_adv, initialized to zero\n",
    "    mask = torch.ones_like(one_hot_adv, dtype=torch.float16)\n",
    "\n",
    "    # Step 4: Use token_ids_init to set the corresponding indices in the mask to 1\n",
    "    # We use gather and scatter operations for this\n",
    "    mask.scatter_(1, top_token_ids_tensor_2d, 0.0)\n",
    "\n",
    "    # Step 5: Apply the mask to one_hot_adv\n",
    "    one_hot_adv = one_hot_adv * mask\n",
    "    \n",
    "    return one_hot_adv\n",
    "\n",
    "# # Example Usage\n",
    "# one_hot_adv = select_only_ASCII_token_ids()\n",
    "# print(one_hot_adv)\n",
    "\n",
    "# sum = torch.sum(one_hot_adv, dim=1)\n",
    "# max = torch.max(one_hot_adv, dim=1)\n",
    "# min = torch.min(one_hot_adv, dim=1)\n",
    "# print(sum,',', max, ',', min)\n",
    "\n",
    "def count_common_words(str1, str2):\n",
    "    # Split the strings into words\n",
    "    words1 = set(str1.split())\n",
    "    words2 = set(str2.split())\n",
    "\n",
    "    # Find the intersection of the two sets to get the common words\n",
    "    common_words = words1.intersection(words2)\n",
    "\n",
    "    # Return the number of common words\n",
    "    return len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_clipped_embedding(embed_weights, generated_embeddings):\n",
    "#     # Step 1: Compute the mean embedding across all samples (dim=0 to get mean for each dimension)\n",
    "#     mean_embedding = torch.mean(embed_weights, dim=0)\n",
    "#     # Step 2: Compute the variance for each dimension\n",
    "#     variance_embedding = torch.var(embed_weights, dim=0, unbiased=False)\n",
    "#     # Compute the standard deviation for each dimension\n",
    "#     std_devs = torch.sqrt(variance_embedding)\n",
    "#     # Set clipping thresholds based on the mean and standard deviations\n",
    "#     lower_thresholds = mean_embedding - 3 * std_devs\n",
    "#     upper_thresholds = mean_embedding + 3 * std_devs\n",
    "#     # Example generated embeddings (replace with your actual data)\n",
    "#     # Clip the generated embeddings based on the thresholds for each dimension\n",
    "#     clipped_embeddings = torch.max(torch.min(generated_embeddings, upper_thresholds), lower_thresholds)\n",
    "#     return clipped_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN ATTACK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "dataset_path: str = \"/home/samuel/research/llmattacks/llm-attacks/data/advbench/harmful_behaviors.csv\"\n",
    "if load_dataset:\n",
    "    reader = csv.reader(open(dataset_path, 'r'))\n",
    "    next(reader)\n",
    "else:\n",
    "    print(\"Error!\")\n",
    "harmful_behaviors = list(reader)[0:1]\n",
    "# We can hard-code this safe_target string; probably\n",
    "safe_target: str = \"I cannot fulfill your request. I'm just an AI.\"\n",
    "print(len(harmful_behaviors))\n",
    "for each_behavior in harmful_behaviors:\n",
    "    print(each_behavior)\n",
    "    # user_prompt, target = each_behavior\n",
    "    # print('Query:', user_prompt)\n",
    "    # user_prompt_tokens = get_tokens(user_prompt)    \n",
    "    # _, embeddings_user = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights)\n",
    "    # generated_output =  model.generate(inputs_embeds=embeddings_user, max_length=len(user_prompt_tokens)).squeeze()\n",
    "    # generated_output_string = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "    # print('Response:', generated_output_string,end='\\n\\n\\n')\n",
    "    # each_behavior.append(generated_output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for one Target(Unsafe->Safe); but plot both the losses together.\n",
    "# Update the one_hot encodings using Gradient Descent.\n",
    "# Use random initialization for Adversarial One Hot\n",
    "from math import *\n",
    "import json \n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "# reader = [[user_prompt, target]]\n",
    "optimization_results = []\n",
    "# Set your regularization parameter\n",
    "initial_coefficient = 1e-4\n",
    "final_coefficient = 1e-2\n",
    "# Create the following directories if they do NOT exist\n",
    "directories = [\"./CSV_Files\", \"./JSON_Files\", \"./TORCH_Files\"]\n",
    "for directory in directories:\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "# JSON file to dump the model generated outputs \n",
    "json_file_path = \"./JSON_Files/output_EGD_w_Entropy(only_ASCII_init)_\"+str(len(harmful_behaviors))+\"_behaviors(\"+str(num_steps)+\"Iterations).json\"\n",
    "\n",
    "for row in tqdm(harmful_behaviors, desc=\"Optimizing prompts\"):\n",
    "    iteration_result = {}\n",
    "    user_prompt, unsafe_target = row\n",
    "    print(user_prompt)\n",
    "    target = unsafe_target\n",
    "    user_prompt_tokens = get_tokens(user_prompt)    \n",
    "    target_tokens = get_tokens(target)[1:]\n",
    "    plotlosses = PlotLosses(groups={'loss': ['unsafe_loss', 'safe_loss']})\n",
    "    # Create both embeddings and one_hot for Safe and Unsafe target Seperate (To plot the losses)\n",
    "    unsafe_target_tokens = get_tokens(unsafe_target)[1:]    \n",
    "    safe_target_tokens = get_tokens(safe_target)[1:]\n",
    "    # adv_suffix_tokens = get_tokens(adv_suffix)[1:]\n",
    "    one_hot_unsafe_target, embeddings_unsafe_target = create_one_hot_and_embeddings(unsafe_target_tokens, embed_weights)\n",
    "    one_hot_safe_target, embeddings_safe_target = create_one_hot_and_embeddings(safe_target_tokens, embed_weights)\n",
    "\n",
    "    # Initialize one_hot encodings and embedding vectors for the user prompts and targets \n",
    "    one_hot_inputs, embeddings_user = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights)\n",
    "    one_hot_target, embeddings_target = create_one_hot_and_embeddings(target_tokens, embed_weights)\n",
    "    best_disc_loss = np.inf\n",
    "    best_loss_at_epoch = 0\n",
    "    # Initialize the adversarial one_hot encodings using ONLY ASCII tokens\n",
    "    one_hot_adv = get_only_ASCII_one_hot_adv()\n",
    "    # Alternatively, initialize adversarial one_hot encodings using adv_suffix_init; This gives a Constant Loss value.\n",
    "    # one_hot_adv, embeddings_adv = create_one_hot_and_embeddings(adv_suffix_tokens, embed_weights) \n",
    "    effective_adv_one_hot = one_hot_adv.detach()\n",
    "    effective_adv_embedding = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "    one_hot_adv.requires_grad_()\n",
    "    # Initialize Adam optimizer with user-defined epsilon value\n",
    "    optimizer = optim.Adam([one_hot_adv], lr=step_size, eps=1e-4) \n",
    "    scheduler_cycle = 0\n",
    "    # Specify the filename\n",
    "    last_three_words = lambda text: '_'.join(text.split()[-3:])\n",
    "    first_three_words = lambda text: '_'.join(text.split()[0:3])\n",
    "    csv_filename = './CSV_Files/output_EGD_w_Entropy(only_ASCII_init)_'+last_three_words(user_prompt)+'('+str(num_steps)+'_iterations).csv'        \n",
    "    # Generate column names\n",
    "    column_names = ['epoch', 'cycle', 'learning_rate', \"entropy_term\", \"unsafe_loss\", \"safe_loss\"] \n",
    "                    # 'continuous_loss','discrete_loss'] \n",
    "    #                'l1_loss', 'total_loss', \n",
    "    # Adding 'max_1' to 'max_20' column names using a loop\n",
    "    for i in range(1, 21):\n",
    "        column_names.append(f'max_{i}')\n",
    "    for i in range(1, 21):\n",
    "        column_names.append(f'token_id_{i}')\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    PHASE_2: bool=False\n",
    "    BREAK_IT: bool=False\n",
    "\n",
    "    for epoch_no in tqdm(range(num_steps)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings_adv = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "        ce_loss, logits = calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, one_hot_target)\n",
    "        continuous_loss = ce_loss.detach().cpu().item()\n",
    "        # Break the loop if loss becomes \"NaN\"\n",
    "        if math.isnan(continuous_loss):\n",
    "            print(\"The value is NaN.\")\n",
    "            BREAK_IT=True # Go to the Next Behavior\n",
    "        # Use the generate() fuction to check whether attack is successful.\n",
    "        tokens_pred = logits.argmax(2)\n",
    "        word_count = len(set(target.split()))\n",
    "        output_str = tokenizer.decode(tokens_pred[0][:].cpu().numpy())\n",
    "        common_word_count = count_common_words(output_str, target)\n",
    "        sucess = common_word_count>=(word_count-1) and continuous_loss<0.2\n",
    "        # OR: sucess = common_word_count>=word_count or continuous_loss<0.2\n",
    "        print(\"common_word_count:\",common_word_count,end=\" for \")\n",
    "        print(\"output_str:\", output_str)\n",
    "        if sucess:        \n",
    "            # clipped_embeddings_adv = get_clipped_embedding(embed_weights, embeddings_adv)\n",
    "            final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv])\n",
    "            # final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv])\n",
    "            outputs = []\n",
    "            generation_loop = range(5)\n",
    "            for loop in tqdm(generation_loop, desc=\"Generating outputs\", leave=False):\n",
    "                generated_output =  model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens).squeeze()\n",
    "                # generated_output =  model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens, do_sample=False).squeeze()\n",
    "                generated_output_string = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "                outputs.append(generated_output_string)\n",
    "\n",
    "            iteration_result = {\n",
    "                \"harmful-behaviour\": user_prompt,\n",
    "                \"target\": target,\n",
    "                \"epoch_no\": epoch_no,\n",
    "                \"continuous_loss\": continuous_loss,\n",
    "                \"outputs\": outputs\n",
    "            }\n",
    "            optimization_results.append(iteration_result)\n",
    "            # Update the JSON File\n",
    "            with open(json_file_path, \"w\") as f:\n",
    "                json.dump(optimization_results, f, indent=4)\n",
    "            # Save the Tensor/vector you have at this point\n",
    "            # Go to the next phase\n",
    "            if PHASE_2==False:\n",
    "                # Save the Tensor/vector you have at this point\n",
    "                torch_file_name = './TORCH_Files/EGD_w_Entropy_Unsafe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(unsafe_target)+'.pt'                \n",
    "                torch.save(one_hot_adv.data, torch_file_name)\n",
    "                target = safe_target\n",
    "                target_tokens = get_tokens(target)[1:]\n",
    "                one_hot_target, embeddings_target = create_one_hot_and_embeddings(target_tokens, embed_weights)\n",
    "                PHASE_2=True\n",
    "            else : # PHASE_2 is True\n",
    "                torch_file_name = './TORCH_Files/EGD_w_Entropy_Safe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(safe_target)+'.pt'                \n",
    "                torch.save(one_hot_adv.data, torch_file_name)\n",
    "                BREAK_IT=True\n",
    "        # Use the generate() fuction to check whether attack is successful.\n",
    "        # ce_loss.backward()\n",
    "        ################# Usual entropy #################\n",
    "        # Calculate H(X) = − ∑_{i=1}^{L} ∑_{j=1}^{|T|} X_{ij} (log X_{ij} −1)\n",
    "        # Adding a small epsilon to avoid log(0) which is undefined\n",
    "        eps = 1e-12\n",
    "        log_one_hot = torch.log(one_hot_adv.to(dtype=torch.float32) + eps)\n",
    "        # Compute the modified entropy\n",
    "        entropy_per_row = -one_hot_adv.to(dtype=torch.float32) * (log_one_hot - 1)\n",
    "        entropy_term = entropy_per_row.sum()\n",
    "        # Calculate the annealed coefficient, 𝜖\n",
    "        # Use Exponential Scheduling\n",
    "        reg_coefficient = initial_coefficient * (final_coefficient / initial_coefficient) ** (epoch_no / (num_steps - 1))   \n",
    "        entropy_term *= reg_coefficient \n",
    " \n",
    "        # Regularized loss: F(X) - epsilon * H(X)\n",
    "        regularized_loss = ce_loss - entropy_term\n",
    "        # Backpropagate the regularized loss\n",
    "        regularized_loss.backward()\n",
    "\n",
    "        # Clip the gradients(before update)\n",
    "        # torch.nn.utils.clip_grad_norm_([one_hot_adv], max_norm=1.0)\n",
    "\n",
    "        # Copy the gradients\n",
    "        grads = one_hot_adv.grad.clone()        \n",
    "        # Do NOT Normalize gradients\n",
    "        # grads = grads / grads.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Exponentiated Gradient Descent update\n",
    "        with torch.no_grad():\n",
    "            # one_hot_adv.data *= torch.exp(-eta * grads)\n",
    "            one_hot_adv.data *= torch.exp(-step_size * grads)\n",
    "            one_hot_adv.data /= one_hot_adv.data.sum(dim=1, keepdim=True)\n",
    "\n",
    "        ## Convex Check\n",
    "        sum = torch.sum(one_hot_adv, dim=1)\n",
    "        max = torch.max(one_hot_adv, dim=1)\n",
    "        non_zero_count = torch.count_nonzero(one_hot_adv, dim=1)\n",
    "\n",
    "        # Clip the gradients(after update)\n",
    "        torch.nn.utils.clip_grad_norm_([one_hot_adv], max_norm=1.0)\n",
    "        # Rest the gradients\n",
    "        model.zero_grad()\n",
    "        one_hot_adv.grad.zero_()\n",
    "        \n",
    "        # # Discretization part\n",
    "        max_values = torch.max(one_hot_adv, dim=1)\n",
    "        adv_token_ids = max_values.indices\n",
    "        # one_hot_discrete = torch.zeros(\n",
    "        #     adv_token_ids.shape[0], embed_weights.shape[0], device=device, dtype=embed_weights.dtype\n",
    "        # )\n",
    "        # one_hot_discrete.scatter_(\n",
    "        #     1,\n",
    "        #     adv_token_ids.unsqueeze(1),\n",
    "        #     torch.ones(one_hot_adv.shape[0], 1, device=device, dtype=embed_weights.dtype),\n",
    "        # )\n",
    "        # Update plotlosses with both Unsafe and Safe loss\n",
    "        ce_loss, _ = calc_loss(model, embeddings_user, embeddings_adv, embeddings_unsafe_target, one_hot_unsafe_target)\n",
    "        unsafe_loss = ce_loss.detach().cpu().item()\n",
    "        ce_loss, _ = calc_loss(model, embeddings_user, embeddings_adv, embeddings_safe_target, one_hot_safe_target)\n",
    "        safe_loss = ce_loss.detach().cpu().item()\n",
    "        plotlosses.update({\n",
    "            \"unsafe_loss\": unsafe_loss,\n",
    "            \"safe_loss\": safe_loss\n",
    "        })\n",
    "        plotlosses.send()\n",
    "\n",
    "        # # Dump Ouput values to a CSV file\n",
    "        # # Convert max_values to a NumPy array\n",
    "        max_values_array = max_values.values.detach().cpu().numpy()\n",
    "        token_ids_array = adv_token_ids.detach().cpu().numpy()\n",
    "        # Compute Cycle using the scheduler's state\n",
    "        # if scheduler_state['T_cur'] == 0:\n",
    "        #         scheduler_cycle += 1\n",
    "        # Create the Initial array       \n",
    "        prepend_array = np.array([epoch_no, scheduler_cycle, step_size,\n",
    "                                  entropy_term.detach().cpu().item(), \n",
    "                                  unsafe_loss, safe_loss]) \n",
    "        \n",
    "        # Concatenate the arrays\n",
    "        row = np.concatenate((prepend_array, max_values_array, token_ids_array))\n",
    "        new_row = pd.Series(row, index=df.columns)\n",
    "        df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "        # Save log data to CSV file periodically\n",
    "        if epoch_no % 10 == 0:\n",
    "            df.to_csv(csv_filename, index=False)\n",
    "        # Something went wrong; so break it and go to the next behavior.\n",
    "        if BREAK_IT==True:\n",
    "            break\n",
    "    # End of optimizations        \n",
    "    # Write to CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths to the .pt files\n",
    "\n",
    "for row in harmful_behaviors:\n",
    "    iteration_result = {}\n",
    "    user_prompt, _ = row\n",
    "    print(f'\\n\\n{user_prompt}')\n",
    "    print('________________________________________________________________________________')\n",
    "    tensor_file_unsafe = './TORCH_Files/EGD_w_Entropy_Unsafe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(unsafe_target)+'.pt'\n",
    "    tensor_file_safe = './TORCH_Files/EGD_w_Entropy_Safe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(safe_target)+'.pt'\n",
    "    tensor_filenames = [tensor_file_unsafe, tensor_file_safe]\n",
    "    loaded_tensors = []\n",
    "    skip: bool=False\n",
    "    for filename in tensor_filenames:\n",
    "        if os.path.exists(filename):\n",
    "            loaded_tensors.append(torch.load(filename))\n",
    "        else:\n",
    "            print(f\"File {filename} does not exist. Skipping...\")\n",
    "            skip=True\n",
    "    if skip==True: # GO to the NExt Iteration\n",
    "        continue\n",
    "    # Load the tensors from the .pt files\n",
    "    # loaded_tensors = [torch.load(filename) for filename in tensor_filenames]\n",
    "\n",
    "    # Now, loaded_tensors is a list containing the loaded tensors\n",
    "    tensor1 = loaded_tensors[0]\n",
    "    tensor2 = loaded_tensors[1]\n",
    "\n",
    "    # Move tensors to CPU and convert to float32 if necessary\n",
    "    tensor1_cpu = tensor1.cpu().float()\n",
    "    tensor2_cpu = tensor2.cpu().float()\n",
    "\n",
    "    # Example usage: Compute cosine similarity and L2 distance\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = F.cosine_similarity(tensor1_cpu, tensor2_cpu, dim=1)\n",
    "\n",
    "    # Compute L2 distance using torch.cdist\n",
    "    l2_distance = torch.cdist(tensor1_cpu.unsqueeze(0), tensor2_cpu.unsqueeze(0), p=2).squeeze(0)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Cosine Similarity:\")\n",
    "    print(cosine_similarity)\n",
    "    print(\"\\nL2-norm Distance size\")\n",
    "    print(l2_distance.shape)\n",
    "\n",
    "    # If you only need the distance between corresponding rows\n",
    "    diagonal_l2_distance = l2_distance.diag()\n",
    "    print(\"L2-norm Distance between corresponding rows:\")\n",
    "    print(diagonal_l2_distance)\n",
    "\n",
    "    # Compute L1 distance\n",
    "    # Compute the L1-norm distance\n",
    "    l1_distance = torch.cdist(tensor1_cpu.unsqueeze(0), tensor2_cpu.unsqueeze(0), p=1).squeeze(0)\n",
    "    # torch.abs(tensor1 - tensor2).sum()\n",
    "    print(\"\\nL1-norm distance size:\\n\", l1_distance.shape)\n",
    "    print(\"L1-norm Distance between corresponding rows:\")\n",
    "    print(l1_distance.diag())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the tensor as a CSV file\n",
    "# def save_tensor_to_csv(tensor, filename):\n",
    "#     # Convert the tensor to a NumPy array\n",
    "#     numpy_array = tensor.cpu().numpy()\n",
    "#     # Convert the NumPy array to a DataFrame\n",
    "#     df = pd.DataFrame(numpy_array)\n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# csv_file_unsafe = './outputs/Unsafe_1_hot_adv_'+first_three_words(unsafe_target)+'.csv'\n",
    "# csv_file_safe = './outputs/Safe_1_hot_adv_'+first_three_words(safe_target)+'.csv'\n",
    "# save_tensor_to_csv(tensor1_cpu, csv_file_unsafe)\n",
    "# save_tensor_to_csv(tensor2_cpu, csv_file_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute cosine_similarity and l2-distance\n",
    "\n",
    "# # Move tensors to CPU if necessary\n",
    "# tensor1_cpu = one_hot_adv_list[0].cpu()\n",
    "# tensor2_cpu = one_hot_adv_list[1].cpu()\n",
    "\n",
    "# # Compute cosine similarity\n",
    "# cosine_similarity = F.cosine_similarity(tensor1_cpu, tensor2_cpu, dim=1)\n",
    "\n",
    "# # Compute L2 distance using torch.cdist\n",
    "# # Move tensors to CPU and convert to float32 if necessary\n",
    "# tensor1_cpu = tensor1_cpu.float()\n",
    "# tensor2_cpu = tensor2_cpu.float()\n",
    "# l2_distance = torch.cdist(tensor1_cpu, tensor2_cpu, p=2)\n",
    "# diagonal_l2_distance = l2_distance.diag()\n",
    "# # Print results\n",
    "# print(\"Cosine Similarity:\")\n",
    "# print(cosine_similarity)\n",
    "# print(\"\\nL2-norm Distance:\")\n",
    "# print(diagonal_l2_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After it's done, compute the similarity and distance between the one_hot_adv_Safe and one_hot_adv_Unsafe\n",
    "# print(len(one_hot_adv_list))\n",
    "# # Use the Last embeddings_adv\n",
    "# for one_hot_adv in one_hot_adv_list:\n",
    "#     embeddings_adv = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "#     final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv])\n",
    "#     outputs = []\n",
    "#     generation_loop = range(10)\n",
    "#     for i in tqdm(generation_loop, desc=\"Generating outputs\", leave=False):\n",
    "#         generated_output =  model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens).squeeze()\n",
    "#         generated_output_string = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "#         print(f\"#{i}\\n\",generated_output_string)\n",
    "#         outputs.append(generated_output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmattacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
