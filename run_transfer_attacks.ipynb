{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a Separate Conda Environment to run this\n",
    "# # from helper import load_harmfulness_evaluator,load_helpfulness_evaluator\n",
    "# import config\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, FalconForCausalLM, MistralForCausalLM, MptForCausalLM\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safe_rlhf.models import AutoModelForScore\n",
    "\n",
    "# # The Following functions are useful for Evaluation\n",
    "# def load_helpfulness_evaluator(model_path, device):\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_path, torch_dtype=torch.bfloat16\n",
    "#     ).to(device).eval()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#     return model, tokenizer\n",
    "\n",
    "# def load_harmfulness_evaluator(model_path, device):\n",
    "#     cost_model = AutoModelForScore.from_pretrained(\n",
    "#         model_path, torch_dtype=torch.bfloat16\n",
    "#     ).to(device).eval()\n",
    "#     cost_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#     return cost_model, cost_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpfulness_model, helpfulness_tokenizer = load_helpfulness_evaluator(model_path=config.llama3_path, device=config.device)\n",
    "# harmfulness_model, harmfulness_tokenizer = load_harmfulness_evaluator(model_path=config.beaver_cost_path, device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import json\n",
    "# import transformers, torch\n",
    "# from behavior import Behavior\n",
    "# # from behavior import Behavior, RRBehavior\n",
    "\n",
    "# print('Evaluating...')\n",
    "\n",
    "# def get_harmbench_eval(behavior):\n",
    "#     # Get the harmbench evaluation for the behavior\n",
    "#     evaluator_input = config.ENTAILMENT_PROMPT.format(behavior=behavior.harmful_behavior, generation=behavior.response)\n",
    "#     # Setting max new tokens as 1, if set longer we can get the explanation but we currently do not need it\n",
    "#     pipeline = transformers.pipeline(\"text-generation\", model=helpfulness_model, tokenizer=helpfulness_tokenizer, model_kwargs={\"torch_dtype\": torch.bfloat16, \"skip_special_tokens\": True}, max_new_tokens = 1, device=config.device, pad_token_id = 128001, eos_token_id = 128001)\n",
    "#     output = pipeline(evaluator_input, return_full_text=False)\n",
    "#     generated_text = output[0]['generated_text']\n",
    "#     return generated_text.startswith(\"Yes\")\n",
    "\n",
    "# def get_beaver_score(behavior):\n",
    "#     # Get the beaver cost score for the behavior\n",
    "#     beaver_input = config.BEAVER_PROMPT.format(harmful_behavior=behavior.harmful_behavior, generation=behavior.response)\n",
    "#     input_ids = harmfulness_tokenizer(beaver_input, return_tensors='pt').to(config.device)\n",
    "#     beaver_output = harmfulness_model(**input_ids).end_scores\n",
    "#     return beaver_output.item()\n",
    "\n",
    "# def process_behavior(behavior):\n",
    "#     # Process the line and return the updated data\n",
    "#     behavior.harmbench_eval = get_harmbench_eval(behavior)\n",
    "#     behavior.beaver_cost_score = get_beaver_score(behavior)\n",
    "#     return behavior\n",
    "\n",
    "# # Process each file in the specified directory\n",
    "# model_name = 'MPT'\n",
    "# dataset_name = 'MaliciousInstruct'\n",
    "# director_path = f'./JSON_Files/{model_name}/{dataset_name}'\n",
    "# for file_path in glob.glob(f'{director_path}/EGD_with_Adam_{model_name}(50_behaviors)(200_steps).jsonl'):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         behaviors = []\n",
    "#         count5 = 0\n",
    "#         count10 = 0\n",
    "#         for line in f:\n",
    "#             data = json.loads(line.strip())\n",
    "#             behavior = Behavior(\n",
    "#                 harmful_behavior=data.get(\"harmful_behavior\", \"\"),\n",
    "#                 suffix=data.get(\"suffix\", \"\"),\n",
    "#                 response=data.get(\"response\", \"\"),\n",
    "#                 harmbench_eval=data.get(\"harmbench_eval\", \"\"),\n",
    "#                 beaver_cost_score=data.get(\"beaver_cost_score\", \"\")\n",
    "#             )\n",
    "#             # behavior = Behavior(\n",
    "#             #     harmful_behavior=data.get(\"harmful_behavior\", \"\"),\n",
    "#             #     suffix=data.get(\"suffix\", \"\"),\n",
    "#             #     response=data.get(\"response\", \"\"),\n",
    "#             #     harmbench_eval=data.get(\"harmbench_eval\", \"\"),\n",
    "#             #     beaver_cost_score=data.get(\"beaver_cost_score\", \"\"),\n",
    "#             #     mean_distance=data.get(\"mean_distance\", \"\")\n",
    "#             # )\n",
    "#             if not file_path.endswith('_processed.jsonl'):\n",
    "#                 behavior = process_behavior(behavior)\n",
    "#             # Count behaviors with harmbench_eval = True and beaver_cost_score > 10\n",
    "#             count5 = sum(1 for behavior in behaviors if behavior.harmbench_eval and behavior.beaver_cost_score > 5)\n",
    "#             count10 = sum(1 for behavior in behaviors if behavior.harmbench_eval and behavior.beaver_cost_score > 10)\n",
    "#             behaviors.append(behavior)\n",
    "#         # mean_distance = sum(behavior.mean_distance for behavior in behaviors if behavior.mean_distance and behavior.mean_distance != float('inf')) / len(behaviors)\n",
    "\n",
    "    \n",
    "#     print(f\"File: {file_path}, Count > 5: {count5}, Count > 10: {count10}\")\n",
    "#     # print(f\"File: {file_path}, Count > 5: {count5}, Count > 10: {count10}, Mean Distance: {mean_distance}\")\n",
    "\n",
    "#     if not file_path.endswith('_processed.jsonl'):\n",
    "#         output_file_path = file_path.replace('.jsonl', '_processed.jsonl')\n",
    "#         with open(output_file_path, 'w') as f:\n",
    "#             for behavior in behaviors:\n",
    "#                 f.write(json.dumps(behavior.to_dict()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "from helper import get_model_path, load_model_and_tokenizer, get_embedding_matrix, get_tokens, create_one_hot_and_embeddings\n",
    "\n",
    "device: str = \"cuda:0\"\n",
    "num_steps: int = 200\n",
    "num_tokens: int = 300 \n",
    "step_size: float = 0.1\n",
    "\n",
    "seed: int = 42\n",
    "load_csv_data = True\n",
    "# verbose = True\n",
    "# early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Config and Samuel's directory to load different kinds of models\n",
    "# MODEL PATHS    0           1       2       3           4\n",
    "model_options = [\"Falcon\", \"Llama2\", \"MPT\", \"Vicuna\", \"Mistral\"] \n",
    "model_name = model_options[0]\n",
    "model_path: str = get_model_path(model_name=model_name)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        model_path, low_cpu_mem_usage=True, use_cache=False, device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# embed_weights = get_embedding_matrix(model)\n",
    "# # Datset options:       0           1               2                   3\n",
    "# dataset_options = ['AdvBench', 'HarmBench', 'JailbreakBench', 'MaliciousInstruct']\n",
    "# dataset_name = dataset_options[0]\n",
    "# dataset_path: str = f\"./data/{dataset_name}/harmful_behaviors.csv\"\n",
    "# if load_csv_data:\n",
    "#     reader = csv.reader(open(dataset_path, 'r'))\n",
    "#     next(reader)\n",
    "# else:\n",
    "#     print(\"Error!\")\n",
    "# harmful_behaviors = list(reader)[0:2]\n",
    "# # We can hard-code this safe_target string; probably\n",
    "# # safe_target: str = \"I cannot fulfill your request. I'm just an AI.\"\n",
    "# print(len(harmful_behaviors))\n",
    "# for each_behavior in harmful_behaviors:\n",
    "#     print(each_behavior)Ã¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'model_name: {model_name}')\n",
    "print(f'dataset_name: {dataset_name}')\n",
    "print(f'vocabulary size: {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model_name = \"Llama2\" # \"Vicuna\"\n",
    "print(f'source_model_name: {source_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "# import transformers, torch\n",
    "from behavior import Behavior\n",
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*`do_sample` is set to `False`. However, `temperature` is set to.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*`do_sample` is set to `False`. However, `top_p` is set to.*\")\n",
    "# from behavior import Behavior, RRBehavior\n",
    "# Create the following directories if they do NOT exist\n",
    "output_directories = [f'./JSON_Files/Xfer_attacks/{source_model_name}',                    # 0\n",
    "                    f'./JSON_Files/Xfer_attacks/{source_model_name}/{dataset_name}']     # 1\n",
    "for directory in output_directories:\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "# JSON file to dump the model generated outputs \n",
    "# json_file_path = directories[0]+\"/output_EGD_with_Adam_Optimizer(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).json\"\n",
    "# Process each file in the specified directory\n",
    "input_directory = f'./JSON_Files/{source_model_name}/{dataset_name}'\n",
    "for file_path in glob.glob(f'{input_directory}/EGD_with_Adam_{source_model_name}(50_behaviors)(200_steps).jsonl'):\n",
    "    with open(file_path, 'r') as input_file:\n",
    "        for line in input_file:\n",
    "            data = json.loads(line.strip())\n",
    "            behavior = Behavior(\n",
    "                harmful_behavior=data.get(\"harmful_behavior\", \"\"),\n",
    "                suffix=data.get(\"suffix\", \"\"),\n",
    "                response=data.get(\"response\", \"\"),\n",
    "                harmbench_eval=data.get(\"harmbench_eval\", \"\"),\n",
    "                beaver_cost_score=data.get(\"beaver_cost_score\", \"\")\n",
    "            )\n",
    "            # We only need harmful_behavior and suffix to produce output\n",
    "            # Token and Output Generation Step\n",
    "            user_prompt = behavior.harmful_behavior\n",
    "            adv_suffix = behavior.suffix\n",
    "            # Convert the user_prompt and adv_suffix to token_ids\n",
    "            user_prompt_tokens = get_tokens(user_prompt, tokenizer=tokenizer, device=device)\n",
    "            one_hot_inputs, _ = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights, device=device)\n",
    "            inputs_token_ids = one_hot_inputs.argmax(dim=1)\n",
    "            adv_suffix_tokens = get_tokens(adv_suffix, tokenizer=tokenizer, device=device)\n",
    "            one_hot_adv, _ = create_one_hot_and_embeddings(adv_suffix_tokens, embed_weights, device=device)\n",
    "            adv_token_ids = one_hot_adv.argmax(dim=1)\n",
    "            # put the two token_ids together\n",
    "            final_string_ids = torch.hstack([inputs_token_ids, adv_token_ids])\n",
    "            \n",
    "            ######## Discrete tokens for Output Generation ########\n",
    "            generated_output = model.generate(final_string_ids.unsqueeze(0), \n",
    "                                            max_length=num_tokens, \n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            do_sample=False)\n",
    "            generated_output_string = tokenizer.decode(generated_output[0][:].cpu().numpy(), skip_special_tokens=True).strip()\n",
    "            \n",
    "            ## Create and Update a JSONL file (required for running the parse_results.ipynb script) ##\n",
    "            output_file = output_directories[1]+\"/Xfer_\"+model_name+\"(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).jsonl\"\n",
    "            behavior = Behavior(user_prompt, adv_suffix, generated_output_string, \"\", \"\")\n",
    "            \n",
    "            with open(output_file, 'a') as output_file:\n",
    "                output_file.write(json.dumps(behavior.to_dict()) + '\\n')\n",
    "            output_file.close()  \n",
    "    input_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-embedding-attack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
