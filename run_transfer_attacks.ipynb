{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "from helper import get_model_path, load_model_and_tokenizer, get_embedding_matrix, get_tokens, create_one_hot_and_embeddings\n",
    "\n",
    "device: str = \"cuda:0\"\n",
    "num_steps: int = 200\n",
    "num_tokens: int = 300 \n",
    "step_size: float = 0.1\n",
    "\n",
    "seed: int = 42\n",
    "load_csv_data = True\n",
    "# verbose = True\n",
    "# early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Config and Samuel's directory to load different kinds of models\n",
    "# MODEL PATHS    0           1       2       3           4\n",
    "model_options = [\"Falcon\", \"Llama2\", \"MPT\", \"Vicuna\", \"Mistral\"] \n",
    "model_name = model_options[4]\n",
    "model_path: str = get_model_path(model_name=model_name)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        model_path, low_cpu_mem_usage=True, use_cache=False, device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "# Datset options:       0           1               2                   3\n",
    "dataset_options = ['AdvBench', 'HarmBench', 'JailbreakBench', 'MaliciousInstruct']\n",
    "dataset_name = dataset_options[3]\n",
    "dataset_path: str = f\"./data/{dataset_name}/harmful_behaviors.csv\"\n",
    "if load_csv_data:\n",
    "    reader = csv.reader(open(dataset_path, 'r'))\n",
    "    next(reader)\n",
    "else:\n",
    "    print(\"Error!\")\n",
    "harmful_behaviors = list(reader)[0:50]\n",
    "# # We can hard-code this safe_target string; probably\n",
    "# # safe_target: str = \"I cannot fulfill your request. I'm just an AI.\"\n",
    "# print(len(harmful_behaviors))\n",
    "# for each_behavior in harmful_behaviors:\n",
    "#     print(each_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'model_name: {model_name}')\n",
    "print(f'dataset_name: {dataset_name}')\n",
    "print(f'vocabulary size: {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model_name = \"Vicuna\" # \"Llama2\"\n",
    "print(f'source_model_name: {source_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "# import transformers, torch\n",
    "from behavior import Behavior\n",
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*`do_sample` is set to `False`. However, `temperature` is set to.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*`do_sample` is set to `False`. However, `top_p` is set to.*\")\n",
    "# from behavior import Behavior, RRBehavior\n",
    "# Create the following directories if they do NOT exist\n",
    "output_directories = [f'./JSON_Files/Xfer_attacks/{source_model_name}',                    # 0\n",
    "                    f'./JSON_Files/Xfer_attacks/{source_model_name}/{dataset_name}']     # 1\n",
    "for directory in output_directories:\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "# JSON file to dump the model generated outputs \n",
    "# json_file_path = directories[0]+\"/output_EGD_with_Adam_Optimizer(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).json\"\n",
    "# Process each file in the specified directory\n",
    "input_directory = f'./JSON_Files/{source_model_name}/{dataset_name}'\n",
    "for file_path in glob.glob(f'{input_directory}/EGD_with_Adam_{source_model_name}(50_behaviors)(200_steps).jsonl'):\n",
    "    with open(file_path, 'r') as input_file:\n",
    "        for line in input_file:\n",
    "            data = json.loads(line.strip())\n",
    "            behavior = Behavior(\n",
    "                harmful_behavior=data.get(\"harmful_behavior\", \"\"),\n",
    "                suffix=data.get(\"suffix\", \"\"),\n",
    "                response=data.get(\"response\", \"\"),\n",
    "                harmbench_eval=data.get(\"harmbench_eval\", \"\"),\n",
    "                beaver_cost_score=data.get(\"beaver_cost_score\", \"\")\n",
    "            )\n",
    "            # We only need harmful_behavior and suffix to produce output\n",
    "            # Token and Output Generation Step\n",
    "            user_prompt = behavior.harmful_behavior\n",
    "            adv_suffix = behavior.suffix\n",
    "            # Convert the user_prompt and adv_suffix to token_ids\n",
    "            user_prompt_tokens = get_tokens(user_prompt, tokenizer=tokenizer, device=device)\n",
    "            one_hot_inputs, _ = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights, device=device)\n",
    "            inputs_token_ids = one_hot_inputs.argmax(dim=1)\n",
    "            adv_suffix_tokens = get_tokens(adv_suffix, tokenizer=tokenizer, device=device)\n",
    "            one_hot_adv, _ = create_one_hot_and_embeddings(adv_suffix_tokens, embed_weights, device=device)\n",
    "            adv_token_ids = one_hot_adv.argmax(dim=1)\n",
    "            # put the two token_ids together\n",
    "            final_string_ids = torch.hstack([inputs_token_ids, adv_token_ids])\n",
    "            \n",
    "            ######## Discrete tokens for Output Generation ########\n",
    "            generated_output = model.generate(final_string_ids.unsqueeze(0), \n",
    "                                            max_length=num_tokens, \n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            do_sample=False)\n",
    "            generated_output_string = tokenizer.decode(generated_output[0][:].cpu().numpy(), skip_special_tokens=True).strip()\n",
    "            \n",
    "            ## Create and Update a JSONL file (required for running the parse_results.ipynb script) ##\n",
    "            output_file = output_directories[1]+\"/Xfer_\"+model_name+\"(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).jsonl\"\n",
    "            behavior = Behavior(user_prompt, adv_suffix, generated_output_string, \"\", \"\")\n",
    "            \n",
    "            with open(output_file, 'a') as output_file:\n",
    "                output_file.write(json.dumps(behavior.to_dict()) + '\\n')\n",
    "            output_file.close()  \n",
    "    input_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-embedding-attack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
