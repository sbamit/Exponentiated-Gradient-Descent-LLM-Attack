{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS and GLOBALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm.notebook import tqdm\n",
    "from livelossplot import PlotLosses \n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "# model_path: str = \"/home/sajib/Documents/llm-attacks/llama/llama-2-7b-chat-hf\"\n",
    "# use Samuel's directory instead\n",
    "model_path: str = \"/home/samuel/research/llmattacks/llm-attacks/DIR/llama-2/llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "device: str = \"cuda:0\"\n",
    "num_steps: int = 100\n",
    "num_tokens: int = 100\n",
    "step_size: float = 0.1 # A smaller learning_rate does NOT work!\n",
    "# generate_interval: int = 500\n",
    "seed: int = 42\n",
    "load_dataset = True\n",
    "# verbose = True\n",
    "# early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path, tokenizer_path=None, device=\"cuda:0\", **kwargs):\n",
    "    # from llm-attacks\n",
    "    model = (\n",
    "        AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, trust_remote_code=True, **kwargs\n",
    "        ).to(device).eval()\n",
    "    )\n",
    "\n",
    "    tokenizer_path = model_path if tokenizer_path is None else tokenizer_path\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path, trust_remote_code=True, use_fast=False\n",
    "    )\n",
    "\n",
    "    if \"llama-2\" in tokenizer_path:\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_embedding_matrix(model):\n",
    "    # from llm-attacks\n",
    "    if isinstance(model, LlamaForCausalLM):\n",
    "        return model.model.embed_tokens.weight\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        model_path, low_cpu_mem_usage=True, use_cache=False, device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(input_string):\n",
    "    return torch.tensor(tokenizer(input_string)[\"input_ids\"], device=device)\n",
    "\n",
    "\n",
    "def create_one_hot_and_embeddings(tokens, embed_weights):\n",
    "    one_hot = torch.zeros(\n",
    "        tokens.shape[0], embed_weights.shape[0], device=device, dtype=embed_weights.dtype\n",
    "    )\n",
    "    one_hot.scatter_(\n",
    "        1,\n",
    "        tokens.unsqueeze(1),\n",
    "        torch.ones(one_hot.shape[0], 1, device=device, dtype=embed_weights.dtype),\n",
    "    )\n",
    "    embeddings = (one_hot @ embed_weights).unsqueeze(0).data\n",
    "    return one_hot, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, targets):\n",
    "#     full_embeddings = torch.hstack([embeddings_user, embeddings_adv, embeddings_target])\n",
    "#     logits = model(inputs_embeds=full_embeddings).logits\n",
    "#     loss_slice_start = len(embeddings_user[0]) + len(embeddings_adv[0])\n",
    "#     loss = nn.CrossEntropyLoss()(logits[0, loss_slice_start - 1 : -1, :], targets)\n",
    "#     return loss, logits[:, loss_slice_start-1:, :]\n",
    "#     # return loss, logits[:, loss_slice_start:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A safer version of the calc_loss function.\n",
    "def calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, targets):\n",
    "    full_embeddings = torch.hstack([embeddings_user, embeddings_adv, embeddings_target])\n",
    "    \n",
    "    # Check for NaN or extreme values in the embeddings\n",
    "    if torch.isnan(full_embeddings).any():\n",
    "        raise ValueError(\"NaN detected in input embeddings\")\n",
    "    if torch.max(full_embeddings).item() > 1e6 or torch.min(full_embeddings).item() < -1e6:\n",
    "        print(\"Warning: Extreme values detected in embeddings\")\n",
    "\n",
    "    # Pass through model and get logits\n",
    "    logits = model(inputs_embeds=full_embeddings).logits\n",
    "\n",
    "    # Check for NaN or extreme values in logits\n",
    "    if torch.isnan(logits).any():\n",
    "        raise ValueError(\"NaN detected in logits\")\n",
    "    if torch.max(logits).item() > 1e6 or torch.min(logits).item() < -1e6:\n",
    "        print(\"Warning: Extreme values detected in logits\")\n",
    "    \n",
    "    # Apply optional clipping to logits\n",
    "    logits = torch.clamp(logits, min=-1e4, max=1e4)\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss_slice_start = len(embeddings_user[0]) + len(embeddings_adv[0])\n",
    "    loss = nn.CrossEntropyLoss()(logits[0, loss_slice_start - 1 : -1, :], targets)\n",
    "\n",
    "    return loss, logits[:, loss_slice_start - 1:, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN ATTACK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simplex_projection(s):\n",
    "#     \"\"\"\n",
    "#     Project the input tensor s onto the probability simplex.\n",
    "\n",
    "#     Parameters:\n",
    "#     - s: Updated token (PyTorch tensor)\n",
    "\n",
    "#     Returns:\n",
    "#     - p: Projected tensor (PyTorch tensor)\n",
    "#     \"\"\"\n",
    "#     if s.numel() == 0:\n",
    "#         raise ValueError(\"Input tensor s must not be empty\")\n",
    "    \n",
    "#     # Step 1: Sort s into mu in descending order\n",
    "#     mu, _ = torch.sort(s, descending=True)\n",
    "    \n",
    "#     # Step 2: Compute rho\n",
    "#     cumulative_sum = torch.cumsum(mu, dim=0)\n",
    "#     arange = torch.arange(1, s.size(0) + 1, device=s.device)\n",
    "#     condition = mu - (cumulative_sum - 1) / arange > 0\n",
    "#     # Handle numerical precision issues by adding a small epsilon\n",
    "#     if not condition.any():\n",
    "#         epsilon = 1e-10\n",
    "#         condition = (mu + epsilon) - (cumulative_sum - 1) / arange > 0\n",
    "\n",
    "#     # Step 2: Compute rho\n",
    "#     # cumulative_sum = torch.cumsum(mu, dim=0)\n",
    "#     # arange = torch.arange(1, s.size(0) + 1, device=s.device)\n",
    "#     # condition = mu - (cumulative_sum - 1) / arange > 0\n",
    "#     nonzero_indices = torch.nonzero(condition, as_tuple=False)\n",
    "#     if nonzero_indices.size(0) == 0:\n",
    "#         rho = 1.0  # Default rho to 1 if no valid condition is found\n",
    "#     else:\n",
    "#         rho = nonzero_indices[-1].item() + 1.0\n",
    "    \n",
    "#     # Step 3: Compute psi\n",
    "#     psi = (cumulative_sum[rho - 1] - 1) / rho\n",
    "    \n",
    "#     # Step 4: Compute p\n",
    "#     p = torch.clamp(s - psi, min=0)\n",
    "    \n",
    "#     return p\n",
    "\n",
    "def simplex_projection(s, epsilon=1e-10):\n",
    "    if s.numel() == 0:\n",
    "        raise ValueError(\"Input tensor s must not be empty\")\n",
    "    \n",
    "    # Step 1: Sort s into mu in descending order\n",
    "    mu, _ = torch.sort(s, descending=True)\n",
    "    \n",
    "    # Step 2: Compute rho\n",
    "    cumulative_sum = torch.cumsum(mu, dim=0)\n",
    "    arange = torch.arange(1, s.size(0) + 1, device=s.device)\n",
    "    condition = mu - (cumulative_sum - 1 + epsilon) / (arange + epsilon) > 0\n",
    "\n",
    "    nonzero_indices = torch.nonzero(condition, as_tuple=False)\n",
    "    if nonzero_indices.size(0) == 0:\n",
    "        rho = 1\n",
    "    else:\n",
    "        rho = nonzero_indices[-1].item() + 1\n",
    "\n",
    "    # Step 3: Compute psi\n",
    "    psi = (cumulative_sum[rho - 1] - 1) / rho\n",
    "    \n",
    "    # Step 4: Compute p\n",
    "    p = torch.clamp(s - psi, min=0)\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def project_rows_to_simplex(matrix):\n",
    "    \"\"\"\n",
    "    Apply the simplex projection row-wise to a 2D tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix: 2D tensor (PyTorch tensor)\n",
    "\n",
    "    Returns:\n",
    "    - projected_matrix: Row-wise simplex projected 2D tensor (PyTorch tensor)\n",
    "    \"\"\"\n",
    "    projected_matrix = torch.zeros_like(matrix)\n",
    "    for i in range(matrix.size(0)):\n",
    "        projected_matrix[i] = simplex_projection(matrix[i])\n",
    "    return projected_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entropy_projection(s, target_entropy=2):\n",
    "#     \"\"\"\n",
    "#     Project the input tensor s onto the entropy-constrained space.\n",
    "\n",
    "#     Parameters:\n",
    "#     - s: Relaxed token (PyTorch tensor with values in [0, 1])\n",
    "#     - Sq: Target entropy (default is 2)\n",
    "#     - epsilon: Small value to avoid division by zero or log(0)\n",
    "\n",
    "#     Returns:\n",
    "#     - p: Projected tensor (PyTorch tensor)\n",
    "#     \"\"\"\n",
    "#     # Step 1: Compute center c\n",
    "#     mask = (s > 0).float()  # Indicator function I[s > 0]\n",
    "#     non_zero_count = torch.sum(mask)\n",
    "#     c = mask / non_zero_count  # Center c\n",
    "\n",
    "#     # Step 2: Compute radius R\n",
    "#     gini_index = 1 - torch.square(s).sum() # PGD uses 's', NOT 'mask'\n",
    "#     R = torch.sqrt(1.0 - gini_index - (1.0/non_zero_count))\n",
    "#     # Compute Euclidean norm of (s - c)\n",
    "#     norm_s_c = torch.norm(s - c)\n",
    "\n",
    "#     # Step 4: Check if R >= ||s - c||\n",
    "#     if R >= norm_s_c:\n",
    "#         # Step 5: Return s if the condition is satisfied\n",
    "#         return s\n",
    "#     else:\n",
    "#         # Step 7: Project to simplex with scaling\n",
    "#         scaled_s = R / norm_s_c * (s - c) + c\n",
    "#         return simplex_projection(scaled_s)\n",
    "\n",
    "\n",
    "def entropy_projection(s, target_entropy=2, epsilon=1e-10):\n",
    "    mask = (s > epsilon).float()\n",
    "    non_zero_count = torch.sum(mask) + epsilon  # Prevent division by zero\n",
    "    c = mask / non_zero_count\n",
    "\n",
    "    # Step 2: Compute radius R\n",
    "    gini_index = 1 - torch.square(mask).sum()  # Ensure gini_index >= 0 ; PGD uses 's', NOT 'mask'\n",
    "    gini_index = torch.clamp(gini_index, min=0, max=1)  # Keep it in valid range\n",
    "    R = torch.sqrt(1.0 - gini_index + epsilon - (1.0 / non_zero_count))\n",
    "\n",
    "    # Compute Euclidean norm of (s - c)\n",
    "    norm_s_c = torch.norm(s - c)\n",
    "\n",
    "    # Check if R >= ||s - c||\n",
    "    if R >= norm_s_c:\n",
    "        return s\n",
    "    else:\n",
    "        scaled_s = R / (norm_s_c + epsilon) * (s - c) + c\n",
    "        return simplex_projection(scaled_s)\n",
    "\n",
    "\n",
    "def project_rows_to_entropy(matrix):\n",
    "    \"\"\"\n",
    "    Apply the simplex projection row-wise to a 2D tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix: 2D tensor (PyTorch tensor)\n",
    "\n",
    "    Returns:\n",
    "    - projected_matrix: Row-wise simplex projected 2D tensor (PyTorch tensor)\n",
    "    \"\"\"\n",
    "    projected_matrix = torch.zeros_like(matrix)\n",
    "    for i in range(matrix.size(0)):\n",
    "        projected_matrix[i] = entropy_projection(matrix[i])\n",
    "    return projected_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN ATTACK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "dataset_path: str = \"/home/samuel/research/llmattacks/llm-attacks/data/advbench/harmful_behaviors.csv\"\n",
    "if load_dataset:\n",
    "    reader = csv.reader(open(dataset_path, 'r'))\n",
    "    next(reader)\n",
    "else:\n",
    "    print(\"Error!\")\n",
    "harmful_behaviors = list(reader)[0:1]\n",
    "print(len(harmful_behaviors))\n",
    "for each_behavior in harmful_behaviors:\n",
    "    print(each_behavior)\n",
    "    # user_prompt, target = each_behavior\n",
    "    # print('Query:', user_prompt)\n",
    "    # user_prompt_tokens = get_tokens(user_prompt)    \n",
    "    # _, embeddings_user = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights)\n",
    "    # generated_output =  model.generate(inputs_embeds=embeddings_user, max_length=len(user_prompt_tokens)).squeeze()\n",
    "    # generated_output_string = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "    # print('Response:', generated_output_string,end='\\n\\n\\n')\n",
    "    # each_behavior.append(generated_output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam from optim\n",
    "# Update the one_hot encodings using Gradient Descent.\n",
    "# Use random initialization for Adversarial One Hot\n",
    "from math import * \n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "embed_weights = get_embedding_matrix(model)\n",
    "# reader = [[user_prompt, target]]\n",
    "optimization_results = []\n",
    "# Create the following directories if they do NOT exist\n",
    "directories = [\"./CSV_Files\", \"./JSON_Files\", \"./TORCH_Files\"]\n",
    "for directory in directories:\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")\n",
    "# JSON file to dump the model generated outputs \n",
    "json_file_path = \"./JSON_Files/output_PGD_w_Adam_Cosine(\"+str(len(harmful_behaviors))+\"_behaviors)(\"+str(num_steps)+\"_steps).json\"\n",
    "# json_file_path = \"./JSON_Files/output_ProGD_with_Cosine(random_init)_\"+str(len(harmful_behaviors))+\"_behaviors(\"+str(num_steps)+\"Iterations).json\"\n",
    "\n",
    "# for row in reader:\n",
    "for row in tqdm(harmful_behaviors, desc=\"Optimizing prompts\"):\n",
    "    iteration_result = {}\n",
    "    user_prompt, target = row\n",
    "    print(user_prompt)\n",
    "    # adv_string_tokens = get_tokens(adv_string_init)[1:]\n",
    "    user_prompt_tokens = get_tokens(user_prompt)\n",
    "    # target = unsafe_target    \n",
    "    target_tokens = get_tokens(target)[1:]\n",
    "    plotlosses = PlotLosses()\n",
    "    plotlosses = PlotLosses(groups={'loss': ['continuous_loss', 'discrete_loss']})\n",
    "    # Plotloss to graph the loss\n",
    "    # Create both embeddings and one_hot for Safe and Unsafe target Seperate (To plot the losses)\n",
    "    # unsafe_target_tokens = get_tokens(unsafe_target)[1:]    \n",
    "    # safe_target_tokens = get_tokens(safe_target)[1:]\n",
    "    # adv_suffix_tokens = get_tokens(adv_suffix)[1:]\n",
    "    # one_hot_unsafe_target, embeddings_unsafe_target = create_one_hot_and_embeddings(unsafe_target_tokens, embed_weights)\n",
    "    # one_hot_safe_target, embeddings_safe_target = create_one_hot_and_embeddings(safe_target_tokens, embed_weights)\n",
    "    one_hot_inputs, embeddings_user = create_one_hot_and_embeddings(user_prompt_tokens, embed_weights)\n",
    "    # one_hot_adv, _ = create_one_hot_and_embeddings(adv_string_tokens, embed_weights)\n",
    "    one_hot_adv = F.softmax(torch.rand(20, 32000, dtype=torch.float16).to(device=device), dim=1)\n",
    "    one_hot_target, embeddings_target = create_one_hot_and_embeddings(target_tokens, embed_weights)\n",
    "    best_disc_loss = np.inf\n",
    "    # cur_loss_list = []\n",
    "    effective_adv_one_hot = one_hot_adv.detach()\n",
    "    effective_adv_embedding = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "    one_hot_adv.requires_grad_()\n",
    "    # Initialize Adam optimizer with user-defined epsilon value\n",
    "    optimizer = optim.Adam([one_hot_adv], lr=step_size, eps=1e-4)\n",
    "    # Initialize lr scheduler (Cosine Annealing with Warm Restarts)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0, last_epoch=-1)\n",
    "    scheduler_cycle = 0\n",
    "    # Specify the filename\n",
    "    last_three_words = lambda text: '_'.join(text.split()[-3:])\n",
    "    first_three_words = lambda text: '_'.join(text.split()[0:3])\n",
    "    csv_filename = './CSV_Files/output_PGD_w_Adam_Cosine('+first_three_words(user_prompt)+\"..\"+last_three_words(user_prompt)+')('+str(num_steps)+'_steps).csv'\n",
    "    # csv_filename = './CSV_Files/output_ProGD_with_Cosine(random_init)_'+last_three_words(user_prompt)+'('+str(num_steps)+'_iterations).csv'\n",
    "    # Generate column names\n",
    "    column_names = ['epoch', 'cycle', 'learning_rate', 'continuous_loss', 'discrete_loss']\n",
    "                    # \"unsafe_loss\", \"safe_loss\"] \n",
    "                    \n",
    "    # Adding 'max_1' to 'max_20' column names using a loop\n",
    "    for i in range(1, 21):\n",
    "        column_names.append(f'max_{i}')\n",
    "    for i in range(1, 21):\n",
    "        column_names.append(f'token_id_{i}')\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "    # PHASE_2: bool=False\n",
    "    BREAK_IT: bool=False\n",
    "\n",
    "    for epoch_no in tqdm(range(num_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        # model.zero_grad()\n",
    "        embeddings_adv = (one_hot_adv @ embed_weights).unsqueeze(0)\n",
    "        cross_entropy_loss, logits = calc_loss(model, embeddings_user, embeddings_adv, embeddings_target, one_hot_target)\n",
    "        continuous_loss = cross_entropy_loss.detach().cpu().item()\n",
    "        # Break the loop if loss becomes \"NaN\"\n",
    "        if math.isnan(continuous_loss):\n",
    "            print(\"The value is NaN. Skipping behavior.\")\n",
    "            BREAK_IT=True # Go to the Next Behavior\n",
    "\n",
    "        # # Discretization part\n",
    "        max_values = torch.max(one_hot_adv, dim=1)\n",
    "        adv_token_ids = max_values.indices\n",
    "        one_hot_discrete = torch.zeros(\n",
    "            adv_token_ids.shape[0], embed_weights.shape[0], device=device, dtype=embed_weights.dtype\n",
    "        )\n",
    "        one_hot_discrete.scatter_(\n",
    "            1,\n",
    "            adv_token_ids.unsqueeze(1),\n",
    "            torch.ones(one_hot_adv.shape[0], 1, device=device, dtype=embed_weights.dtype),\n",
    "        )\n",
    "        # Use one_hot_discrete to print Tokens\n",
    "        # What other techniques Can we use here to discretize the one_hot encodings?\n",
    "        # print(\"Adversarial tokens: \", tokenizer.decode(one_hot_discrete.argmax(dim=1)) )\n",
    "        # Use discrete tokens to calculate loss\n",
    "        embeddings_adv_discrete = (one_hot_discrete @ embed_weights).unsqueeze(0)\n",
    "        disc_loss, _ = calc_loss(model, embeddings_user, embeddings_adv_discrete, embeddings_target, one_hot_target)\n",
    "        # If loss improves, save it as x_best\n",
    "        discrete_loss =  disc_loss.detach().cpu().item()\n",
    "        # cur_loss_list.append(cur_loss)\n",
    "        if discrete_loss < best_disc_loss:\n",
    "            # print(f\"########## {discrete_loss} #########\")\n",
    "            best_disc_loss = discrete_loss\n",
    "            best_loss_at_epoch = epoch_no\n",
    "            effective_adv_embeddings = embeddings_adv_discrete\n",
    "            effective_adv_one_hot = one_hot_discrete\n",
    "        else :\n",
    "            pass\n",
    "        # # Update plotlosses with both discrete and continuous loss\n",
    "        # plotlosses.update({\n",
    "        #     \"continuous_loss\": continuous_loss,\n",
    "        #     \"discrete_loss\": discrete_loss\n",
    "        # })\n",
    "        # plotlosses.send()\n",
    "        cross_entropy_loss.backward()\n",
    "        # Get the scheduler's state to get learning_rate\n",
    "        scheduler_state = scheduler.state_dict()\n",
    "        # plotlosses.update({\"loss\": loss.detach().cpu().numpy()})\n",
    "        # grad = one_hot_adv.grad.clone()\n",
    "        torch.nn.utils.clip_grad_norm_([one_hot_adv], max_norm=1.0)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # update scheduler\n",
    "        scheduler.step()\n",
    "        # Get the current cycle number somehow\n",
    "        model.zero_grad()\n",
    "        # optimizer.param_groups[0]['lr'] = adjust_learning_rate(lr=step_size, iteration=i)\n",
    "\n",
    "        # Apply Simplex & Entropy\n",
    "        simplices = project_rows_to_simplex(one_hot_adv)\n",
    "        regularized_simplices = project_rows_to_entropy(simplices)\n",
    "        one_hot_adv.data = regularized_simplices.data \n",
    "        \n",
    "        one_hot_adv.grad.zero_()\n",
    "        # Dump Ouput values to a CSV file\n",
    "        # Convert max_values to a NumPy array\n",
    "        max_values_array = max_values.values.detach().cpu().numpy()\n",
    "        token_ids_array = adv_token_ids.detach().cpu().numpy()\n",
    "        # Get the scheduler's state and learning_rate\n",
    "        scheduler_lr = scheduler_state['_last_lr'][0]\n",
    "        # Compute Cycle\n",
    "        if scheduler_state['T_cur'] == 0:\n",
    "                scheduler_cycle += 1\n",
    "        # Create the Initial array       \n",
    "        prepend_array = np.array([epoch_no, scheduler_cycle, scheduler_lr,\n",
    "                                    continuous_loss, discrete_loss]) \n",
    "                                #   unsafe_loss, safe_loss])  \n",
    "        \n",
    "        \n",
    "        # Concatenate the arrays\n",
    "        row = np.concatenate((prepend_array, max_values_array, token_ids_array))\n",
    "        new_row = pd.Series(row, index=df.columns)\n",
    "        df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "        # df = df.append(pd.Series(row, index=df.columns), ignore_index=True)\n",
    "        # # Save log data to CSV file periodically\n",
    "        # if epoch_no % 10 == 0:\n",
    "        #     df.to_csv(csv_filename, index=False)\n",
    "        # Something went wrong; so break it and go to the next behavior.\n",
    "        if BREAK_IT==True:\n",
    "            break\n",
    "    # End of optimizations            \n",
    "    # Write to CSV file\n",
    "    # df.to_csv(csv_filename, index=False)\n",
    "    # Token and Output Generation Step\n",
    "    inputs_token_ids = one_hot_inputs.argmax(dim=1)\n",
    "    adv_token_ids = effective_adv_one_hot.argmax(dim=1)\n",
    "    adv_suffix_list = adv_token_ids.tolist()\n",
    "    final_string_ids = torch.hstack([inputs_token_ids, adv_token_ids])\n",
    "    outputs = []\n",
    "    generation_loop = range(0)\n",
    "    for loop in tqdm(generation_loop, desc=\"Generating outputs\", leave=False):\n",
    "        # generated_output = model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens).squeeze()\n",
    "        # generated_output =  model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens, do_sample=False).squeeze()\n",
    "        # generated_output_string = tokenizer.decode(generated_output, skip_special_tokens=True)\n",
    "        generated_output = model.generate(final_string_ids.unsqueeze(0), max_length=num_tokens, pad_token_id=tokenizer.pad_token_id)\n",
    "        generated_output_string = tokenizer.decode(generated_output[0][1:].cpu().numpy(), skip_special_tokens=True).strip()\n",
    "        outputs.append(generated_output_string)\n",
    "\n",
    "    iteration_result = {\n",
    "        \"harmful-behaviour\": user_prompt,\n",
    "        \"suffix_token_ids\": adv_suffix_list,\n",
    "        \"target\": target,\n",
    "        \"epoch_no\": epoch_no,\n",
    "        \"continuous_loss\": continuous_loss,\n",
    "        \"discrete_loss\": discrete_loss,\n",
    "        \"best_disc_loss\":best_disc_loss,\n",
    "        \"epch_at_best_disc_loss\": best_loss_at_epoch,\n",
    "        \"outputs\": outputs\n",
    "    }\n",
    "    optimization_results.append(iteration_result)\n",
    "    # Update the JSON File\n",
    "    with open(json_file_path, \"w\") as f:\n",
    "        json.dump(optimization_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File paths to the .pt files\n",
    "# # tensor_filenames = ['Safe_one_hot_adv.pt', 'Unsafe_one_hot_adv.pt']\n",
    "# for row in harmful_behaviors:\n",
    "#     iteration_result = {}\n",
    "#     user_prompt, _ = row\n",
    "#     print(f'\\n\\n{user_prompt}')\n",
    "#     print('________________________________________________________________________________')\n",
    "#     tensor_file_unsafe = './TORCH_Files/ProGD_with_Cosine_Unsafe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(unsafe_target)+'.pt'\n",
    "#     tensor_file_safe = './TORCH_Files/ProGD_with_Cosine_Safe_1_hot_adv_'+last_three_words(user_prompt)+'_'+first_three_words(safe_target)+'.pt'\n",
    "#     tensor_filenames = [tensor_file_unsafe, tensor_file_safe]\n",
    "#     # Load the tensors from the .pt files\n",
    "#     loaded_tensors = [torch.load(filename) for filename in tensor_filenames]\n",
    "\n",
    "#     # Now, loaded_tensors is a list containing the loaded tensors\n",
    "#     tensor1 = loaded_tensors[0]\n",
    "#     tensor2 = loaded_tensors[1]\n",
    "\n",
    "#     # Move tensors to CPU and convert to float32 if necessary\n",
    "#     tensor1_cpu = tensor1.cpu().float()\n",
    "#     tensor2_cpu = tensor2.cpu().float()\n",
    "\n",
    "#     # Example usage: Compute cosine similarity and L2 distance\n",
    "#     import torch.nn.functional as F\n",
    "\n",
    "#     # Compute cosine similarity\n",
    "#     cosine_similarity = F.cosine_similarity(tensor1_cpu, tensor2_cpu, dim=1)\n",
    "\n",
    "#     # Compute L2 distance using torch.cdist\n",
    "#     l2_distance = torch.cdist(tensor1_cpu.unsqueeze(0), tensor2_cpu.unsqueeze(0), p=2).squeeze(0)\n",
    "\n",
    "#     # Print results\n",
    "#     print(\"Cosine Similarity:\")\n",
    "#     print(cosine_similarity)\n",
    "#     print(\"\\nL2-norm Distance size\")\n",
    "#     print(l2_distance.shape)\n",
    "\n",
    "#     # If you only need the distance between corresponding rows\n",
    "#     diagonal_l2_distance = l2_distance.diag()\n",
    "#     print(\"L2-norm Distance between corresponding rows:\")\n",
    "#     print(diagonal_l2_distance)\n",
    "\n",
    "#     # Compute L1 distance\n",
    "#     # Compute the L1-norm distance\n",
    "#     l1_distance = torch.cdist(tensor1_cpu.unsqueeze(0), tensor2_cpu.unsqueeze(0), p=1).squeeze(0)\n",
    "#     # torch.abs(tensor1 - tensor2).sum()\n",
    "#     print(\"\\nL1-norm distance size:\\n\", l1_distance.shape)\n",
    "#     print(\"L1-norm Distance between corresponding rows:\")\n",
    "#     print(l1_distance.diag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the tokens twice. \n",
    "# # once according to the effective one_hot then according to the effective_embedding\n",
    "# print(effective_adv_one_hot.shape)\n",
    "# inputs_token_ids = one_hot_inputs.argmax(dim=1)\n",
    "# adv_token_ids =  effective_adv_one_hot.argmax(dim=1)\n",
    "# input_ids = torch.hstack([inputs_token_ids, adv_token_ids])\n",
    "# for token_id in input_ids:\n",
    "#     print(f\"({tokenizer.decode(token_id)})\", end=' ')\n",
    "#     # print(f\"({tokenizer.decode(token_id)}, {token_id.item()})\", end=' ')\n",
    "# print('\\n')\n",
    "\n",
    "# final_prompt_embeds = torch.hstack([embeddings_user, effective_adv_embeddings]).squeeze(0)\n",
    "# for i in range(final_prompt_embeds.shape[0]):\n",
    "#     if i < final_prompt_embeds.shape[0]:\n",
    "#         similarities = torch.nn.functional.cosine_similarity(embed_weights, final_prompt_embeds[i])\n",
    "#         token_id = similarities.argmax().cpu()\n",
    "#         print(f\"({tokenizer.decode(token_id)})\", end=' ')\n",
    "#         # print(f\"({tokenizer.decode(token_id)}, {token_id.item()})\", end=' ')  \n",
    "# print('\\n')\n",
    "\n",
    "# # for i in range(final_prompt_embeds.shape[0]):\n",
    "# #     if i < final_prompt_embeds.shape[0]:\n",
    "# #         similarities = torch.nn.functional.cosine_similarity(embed_weights, final_prompt_embeds[i])\n",
    "# #         token_id = similarities.argmax().cpu()\n",
    "# #         print(f\"({token_id}, {similarities[token_id].cpu().item(): .1f})\",end=' ')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the effective_adv_embeddings & effective_adv_one_hot to test the Attack\n",
    "# # print(\"\\n###### Customized generate() function; Using Adversarial Embeddings ########\\n\")\n",
    "# # final_prompt_embeds = torch.hstack([embeddings_user, effective_adv_embeddings])\n",
    "# # generated_tokens = generate(model, final_prompt_embeds, num_tokens)\n",
    "# # generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "# # print(\"OUTPUT\\n\", generated_text)\n",
    "\n",
    "# # print(\"\\n###### Model.generate() function; Using Adversarial Embeddings ########\\n\")\n",
    "# # # Use the Last embeddings_adv\n",
    "# # final_prompt_embeds = torch.hstack([embeddings_user, embeddings_adv])\n",
    "# # generated_text = model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens).squeeze()\n",
    "# # print(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "\n",
    "# \"\"\"\n",
    "# print(\"\\n###### Model.generate() function; Using Adversarial Embeddings ########\\n\")\n",
    "# final_prompt_embeds = torch.hstack([embeddings_user, effective_adv_embeddings])\n",
    "# for i in range(10):\n",
    "#     print(f\"________________________{i+1}________________________\")\n",
    "#     generated_text = model.generate(inputs_embeds=final_prompt_embeds, max_length=num_tokens).squeeze()\n",
    "#     print(tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "# \"\"\"\n",
    "# inputs_token_ids = one_hot_inputs.argmax(dim=1)\n",
    "# adv_token_ids =  effective_adv_one_hot.argmax(dim=1)\n",
    "# input_ids = torch.hstack([inputs_token_ids, adv_token_ids])\n",
    "# print(\"\\n\\n\\n###### Model.generate() function; Using Adversarial Token IDs ########\\n\")\n",
    "# for i in range(10):\n",
    "#     print(f\"________________________{i+1}________________________\")\n",
    "#     generated_text = model.generate(input_ids.unsqueeze(0), max_length=200)[0]\n",
    "#     print(tokenizer.decode(generated_text, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# s: Updated token (e.g., logits or any real-valued tensor)\n",
    "# s = torch.tensor([0.2, 0.1, -0.1, 0.4, 0.3])\n",
    "# projected_s = simplex_projection(s)\n",
    "# print(projected_s)\n",
    "\n",
    "# def simplex_projection(s):\n",
    "#     s = s.flatten()\n",
    "#     mu, _ = torch.sort(s, descending=True)\n",
    "#     cumulative_sum = torch.cumsum(mu, dim=0) - 1\n",
    "#     rho = torch.sum((mu - cumulative_sum / (torch.arange(1, mu.shape[0] + 1, device=mu.device))) > 0)\n",
    "#     psi = cumulative_sum[rho - 1] / rho\n",
    "#     p = torch.clamp(s - psi, min=0)\n",
    "#     return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# s = torch.tensor([0.4, 0.3, 0.2, 0.1, 0.0])\n",
    "# projected_s = simplex_projection(s)\n",
    "# print('simplex', projected_s)\n",
    "# # s: Relaxed token (e.g., probabilities)\n",
    "# projected_s = project_to_entropy(s)\n",
    "# print('entropy',projected_s)\n",
    "\n",
    "\n",
    "# def entropy_projection(p, target_entropy=2):\n",
    "#   \"\"\"\n",
    "#   Projects a vector onto the simplex with a target entropy.\n",
    "\n",
    "#   Args:\n",
    "#       p: A PyTorch tensor of shape (|T|,) representing the relaxed token probabilities.\n",
    "#       target_entropy: A float representing the desired entropy of the output distribution.\n",
    "\n",
    "#   Returns:\n",
    "#       A PyTorch tensor of shape (|T|,) representing the entropy-regularized probabilities.\n",
    "#   \"\"\"\n",
    "#   # Check if projection is necessary (skip if already within target entropy)\n",
    "#   mask = torch.gt(p, 0)  # Indicator for non-zero elements\n",
    "#   center = torch.div(mask.float(), torch.sum(mask, dim=0))  # Average of non-zero elements\n",
    "#   radius = torch.sqrt(1.0 / (target_entropy - 1.0) * torch.sum(mask, dim=0))  # Radius based on target entropy\n",
    "#   norm = torch.linalg.norm(p - center)  # Norm of the difference\n",
    "#   skip_projection = torch.gt(radius, norm)  # Skip if norm is less than radius\n",
    "\n",
    "#   # Apply simplex projection if necessary\n",
    "#   projected_p = torch.where(skip_projection, p, torch.nn.functional.relu(radius / norm * (p - center) + center))\n",
    "#   output = simplex_projection(projected_p)\n",
    "#   # Ensure the output is the same dtype as the input\n",
    "#   return output.to(p.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmattacks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
